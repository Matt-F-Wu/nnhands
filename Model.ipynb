{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resouces:**\n",
    "\n",
    "Explanation on the resnet architecture: input_size/output_size/kernel/stride at each layer:\n",
    "https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624\n",
    "\n",
    "Resnet50 Architecture:\n",
    "https://www.kaggle.com/keras/resnet50\n",
    "\n",
    "Simple way of unpacking resnetX for stripping out FC layers and such:\n",
    "https://discuss.pytorch.org/t/resnet-pretrained-model-with-last-fc-layer-stripped-does-not-work/17951\n",
    "\n",
    "Reason as to why we want to resize each image and their labels to 224 x 224:\n",
    "https://stackoverflow.com/questions/43922308/what-input-image-size-is-correct-for-the-version-of-resnet-v2-in-tensorflow-slim\n",
    "\n",
    "How to modify the FC layer of resnet:\n",
    "https://discuss.pytorch.org/t/how-to-modify-the-final-fc-layer-based-on-the-torch-model/766/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing requirement:**\n",
    "\n",
    "In order to define the heatmap loss as torch.nn.functional.cross_entropy(input, target, weight=None, size_average=True, ignore_index=-100, reduce=True):\n",
    "\n",
    "We need to have the target/label take on the form of (N, H, W), where the output needs to have (N, 12, H, W).\n",
    "\n",
    "For each label tensor (H, W):\n",
    "Each entry represents which join a location contains: value 1-11 are joints 1 to 11, value 0 represents no-joints at this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d31718c3d7e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoints2d\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_train_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'foo.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    Scale(0.5),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "hand_train = HandDataset('dataset.csv', transform=transform, train=True)\n",
    "N = len(hand_train)\n",
    "loader_train = DataLoader(hand_train, batch_size=64,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8))))\n",
    "\n",
    "hand_val = HandDataset('dataset.csv', transform=transform, train=True)\n",
    "loader_val = DataLoader(hand_val, batch_size=64,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8), int(N*0.9))))\n",
    "\n",
    "hand_test = HandDataset('dataset.csv', transform=transform, train=False)\n",
    "loader_test = DataLoader(hand_test, batch_size=64,\n",
    "                         sampler=sampler.SubsetRandomSampler(range(int(N*0.9),N)))\n",
    "\n",
    "# TODO: read data and create loader=(X_train, y_train)\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for img_path, joints2d, joints3d in raw_train_data:\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width, channels = img.shape\n",
    "    # scale all image to 224 x 224, and scale their corresponding joint positions too!\n",
    "    h_scale = 224.0 / height\n",
    "    w_scale = 224.0 / width\n",
    "    # scale joint positions\n",
    "    # initialize labels to be zero (no joint) for all spatial locations\n",
    "    label = np.zeros((224, 224))\n",
    "    # Process pos2d and add to y_train\n",
    "    for idx, pos2d in enumerate(joints2d):\n",
    "        pos2d[0] = pos2d[0] * w_scale\n",
    "        pos2d[1] = pos2d[1] * h_scale\n",
    "        label[int(pos2d[0]), int(pos2d[1])] = idx + 1\n",
    "    \n",
    "    X_train.append(img)\n",
    "    y_train.append(label)\n",
    "\n",
    "trainDataset = zip(X_train, y_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to C:\\Users\\haoha/.torch\\models\\resnet34-333f7ec4.pth\n",
      "100.0%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cuda.FloatTensor is not enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-65e4323d07d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Use renet 34 for speed, later may use resnet50 as our base NN stucture for joints detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mresnet34\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresnet34\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# don't change/update the pretrained model parameters, only change the final fc layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cuda.FloatTensor is not enabled."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "# Use renet 34 for speed, later may use resnet50 as our base NN stucture for joints detection\n",
    "resnet34 = models.resnet34(pretrained=True).cuda()\n",
    "for param in resnet34.parameters():\n",
    "    # don't change/update the pretrained model parameters, only change the final fc layer\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Upsample using transpose convolution and unpooling\n",
    "# output_padding = 1 is intended to recover proper size\n",
    "transConv1 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=0, output_padding=1)\n",
    "transConv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=0, output_padding=1)\n",
    "transConv3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=0, output_padding=1)\n",
    "# Note: the following layer should have been maxunpooling, kernel 3 x 3, stride 2\n",
    "# torch.nn.MaxUnpool2d() requires modification of the torchvision resnetXX modules\n",
    "# to have the maxpooling layer return indices of max values, try ConvTranspose2d() instead\n",
    "# If this doesn't work, have to and implement custom version of resnet\n",
    "transConv4 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=0, output_padding=1)\n",
    "transConv5 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=7, stride=2, padding=0, output_padding=1) # (224 x 224)\n",
    "\n",
    "# finally, the join prediction Convolutional Layer, filter size 3 x 3, 21 + 1 = 22 filters?\n",
    "# The +1 (filter dimension 11) is used to denote that the location does not correspond to any joints\n",
    "jointPrediction = nn.Conv2d(in_channels=64, out_channels=21, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# stick a bunch of ReLu non-linearity in the upsampling pipeline\n",
    "'''\n",
    "model = nn.Sequential(\n",
    "    *list(resnet34.children())[:-1], \n",
    "    transConv1,\n",
    "    nn.ReLU(),\n",
    "    transConv2,\n",
    "    nn.ReLU(),\n",
    "    transConv3,\n",
    "    nn.ReLU(),\n",
    "    transConv4,\n",
    "    nn.ReLU(),\n",
    "    transConv5,\n",
    "    nn.ReLU(),\n",
    "    jointPrediction\n",
    ")\n",
    "'''\n",
    "\n",
    "# use a toy model to make sure that the location map implementations are correct\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    ")\n",
    "\n",
    "# model.cuda()\n",
    "# .cuda() makes model run on GPU\n",
    "\n",
    "# optimizer using adam\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for X_batch, y_batch in trainDataset:\n",
    "        # y_pred is of size 224 x 224 x 63, and let's say the filter dimension is layed out in the following way:\n",
    "        # dimension 0-20: j1_x, j2_x, j3_x,...j21_x, dimension 21-41: j1_y, j2_y, j3_y, ... \n",
    "        # Above is for conveniently using broadcasting with heatmap element-wise product\n",
    "        # y_pred is the predicted location maps for all 21 joints\n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        heatmap = \n",
    "        \n",
    "        # use heatmap loss defined in VNect\n",
    "        loss = torch.sum(torch.pow(y_batch.heatmap * (y_pred - y_batch.locMap), 2))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Clears the gradients of all optimized torch.Tensor s\n",
    "        optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
