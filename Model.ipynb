{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resouces:**\n",
    "\n",
    "Explanation on the resnet architecture: input_size/output_size/kernel/stride at each layer:\n",
    "https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624\n",
    "\n",
    "Resnet50 Architecture:\n",
    "https://www.kaggle.com/keras/resnet50\n",
    "\n",
    "Simple way of unpacking resnetX for stripping out FC layers and such:\n",
    "https://discuss.pytorch.org/t/resnet-pretrained-model-with-last-fc-layer-stripped-does-not-work/17951\n",
    "\n",
    "Reason as to why we want to resize each image and their labels to 224 x 224:\n",
    "https://stackoverflow.com/questions/43922308/what-input-image-size-is-correct-for-the-version-of-resnet-v2-in-tensorflow-slim\n",
    "\n",
    "How to modify the FC layer of resnet:\n",
    "https://discuss.pytorch.org/t/how-to-modify-the-final-fc-layer-based-on-the-torch-model/766/3\n",
    "\n",
    "How to partially freeze resnet34:\n",
    "https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing requirement:**\n",
    "\n",
    "In order to define the heatmap loss as torch.nn.functional.cross_entropy(input, target, weight=None, size_average=True, ignore_index=-100, reduce=True):\n",
    "\n",
    "We need to have the target/label take on the form of (N, J, H, W)\n",
    "\n",
    "Each j in J represents a joint\n",
    "\n",
    "**Important:** The image is of size (N, 3, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torchvision import transforms, utils\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from libs.data_utils import HandDataset, ToTensor, Scale, GestureDataset\n",
    "from libs.layer_utils import flatten, random_weight, zero_weight\n",
    "from libs.model_utils import (show_joints, makePosList, makeHeatMapOneHot,\n",
    "                              makeMaps, generate_blw, ComputeLoss, get_loss,\n",
    "                              load_model, save_model)\n",
    "from libs.model import model, modelHeatmap, modelLocmap\n",
    "from libs.lit_data import data2d, data3d, data3d2\n",
    "from libs.misc import write_log\n",
    "                              \n",
    "plt.ion() # interactive mode\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    USE_GPU=False\n",
    "print('Using device:', device)\n",
    "\n",
    "batch_size = 2\n",
    "num_joints = 21\n",
    "image_size = 224\n",
    "dtype = torch.float32\n",
    "g_heatmap_size = 9\n",
    "\n",
    "b_idx = np.repeat(np.arange(batch_size), num_joints)\n",
    "b_idx = torch.from_numpy(b_idx).long()\n",
    "j_idx = np.array(list(np.arange(num_joints))*batch_size)\n",
    "j_idx = torch.from_numpy(j_idx).long()\n",
    "\n",
    "params['batch_size'] = batch_size\n",
    "params['num_joints'] = num_joints\n",
    "params['image_size'] = image_size\n",
    "params['dtype'] = dtype\n",
    "params['device'] = device\n",
    "params['USE_GPU'] = USE_GPU\n",
    "params['g_heatmap_size'] = 9\n",
    "params['b_idx'] = b_idx\n",
    "params['j_idx'] = j_idx\n",
    "\n",
    "blw = generate_blw(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Scale(image_size, image_size),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "hand_train = HandDataset('toy_dataset.csv', transform=transform, train=True, device=device)\n",
    "N = len(hand_train)\n",
    "loader_train = DataLoader(hand_train, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.85))),\n",
    "            drop_last=True)\n",
    "\n",
    "hand_val = HandDataset('toy_dataset.csv', transform=transform, train=True)\n",
    "loader_val = DataLoader(hand_val, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.85), int(N*0.9))),\n",
    "                       drop_last=True)\n",
    "\n",
    "hand_test = HandDataset('toy_dataset.csv', transform=transform, train=False)\n",
    "loader_test = DataLoader(hand_test, batch_size=batch_size,\n",
    "                         sampler=sampler.SubsetRandomSampler(range(int(N*0.9),N)),\n",
    "                        drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "### Define additional params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 600\n",
    "save_every = 1200\n",
    "\n",
    "trainlog_fp = 'trainlog.txt'\n",
    "vallog_fp = 'vallog_fp.txt'\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
    "        list(model.parameters()) + list(modelHeatmap.parameters()) +\\\n",
    "        list(modelLocmap.parameters()) ), lr=1.0e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iter: 0, train Loss: 16.56970977783203\n",
      "Epoch: 0, Iter: 0, val Loss: 16.57006072998047\n",
      "Epoch: 0, Iter: 1, train Loss: 16.547908782958984\n",
      "Epoch: 0, Iter: 2, train Loss: 16.543930053710938\n",
      "Epoch: 0, Iter: 3, train Loss: 16.557571411132812\n",
      "Epoch: 0, Iter: 4, train Loss: 16.516246795654297\n",
      "Epoch: 0, Iter: 5, train Loss: 16.446765899658203\n",
      "Epoch: 0, Iter: 6, train Loss: 16.400293350219727\n",
      "Epoch: 0, Iter: 7, train Loss: 16.299009323120117\n",
      "Epoch: 0, Iter: 8, train Loss: 15.96552562713623\n",
      "Epoch: 0, Iter: 9, train Loss: 16.488601684570312\n",
      "Epoch: 0, Iter: 10, train Loss: 16.14129638671875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8aa98e6b23a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         loss, loss_detailed = get_loss(model, modelHeatmap, modelLocmap,\n\u001b[0;32m----> 9\u001b[0;31m                         image, pos2d_list, pos3d_list, blw, params)\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs231n/nnhands/libs/model_utils.py\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(model, modelHeatmap, modelLocmap, image, pos2d_list, pos3d_list, blw, params)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mh_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelHeatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0ml_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelLocmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_detailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComputeLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_detailed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    for idx, batch in enumerate(loader_train):\n",
    "        image = batch['image'].float()\n",
    "        pos2d_list = batch['pos_2d'] \n",
    "        pos3d_list = batch['pos_3d']\n",
    "        \n",
    "        model.train()\n",
    "        loss, loss_detailed = get_loss(model, modelHeatmap, modelLocmap,\n",
    "                        image, pos2d_list, pos3d_list, blw, params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        write_log(trainlog_fp, epoch, idx, loss, loss_detailed, 'train')\n",
    "        \n",
    "        if idx % print_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = []\n",
    "                val_loss_det = []\n",
    "                for vidx, vbatch in enumerate(loader_val):\n",
    "                    image = vbatch['image'].float()\n",
    "                    pos2d_list = vbatch['pos_2d'] \n",
    "                    pos3d_list = vbatch['pos_3d']\n",
    "                    \n",
    "                    vloss, vloss_detailed = get_loss(model, modelHeatmap, modelLocmap,\n",
    "                        image, pos2d_list, pos3d_list, blw, params)\n",
    "                    \n",
    "                    val_loss.append(vloss)\n",
    "                    val_loss_det.append(vloss_detailed)\n",
    "                    \n",
    "            val_loss = np.mean(val_loss)\n",
    "            val_loss_det = np.array(val_loss_det)\n",
    "            val_loss_det = np.mean(val_loss_det, axis=0)\n",
    "            write_log(vallog_fp, epoch, idx, val_loss, val_loss_det, 'val')\n",
    "                \n",
    "        if idx % save_every == 0:\n",
    "            save_model(epoch, idx, model, modelHeatmap, modelLocmap, optimizer)\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePosList(h_pred, l_pred):\n",
    "    p2d_y, p2d_x = np.unravel_index(torch.argmax(h_pred.view(num_joints, -1), dim=1).data.numpy(), (image_size, image_size))\n",
    "    p2d = np.stack((p2d_x, p2d_y), axis=-1)\n",
    "\n",
    "    p3d_x = l_pred[0].data.numpy()\n",
    "    p3d_x = p3d_x[p2d_y, p2d_x]\n",
    "\n",
    "    p3d_y = l_pred[1].data.numpy()\n",
    "    p3d_y = p3d_y[p2d_y, p2d_x]\n",
    "\n",
    "    p3d_z = l_pred[2].data.numpy()\n",
    "    p3d_z = p3d_z[p2d_y, p2d_x]\n",
    "\n",
    "    p3d = np.stack((p3d_x, p3d_y, p3d_z), axis=-1)\n",
    "    \n",
    "    return p2d, p3d\n",
    "#b_idx = torch.from_numpy(np.repeat(np.arange(batch_size), num_joints)).long()\n",
    "mplb_idx = np.repeat(np.arange(batch_size), num_joints)\n",
    "def makePosListBatch(h_pred, l_pred):\n",
    "    idx_2d = torch.argmax(h_pred.view(batch_size, num_joints, -1), dim=2).data.numpy()\n",
    "    \n",
    "    p2d_y, p2d_x = np.unravel_index(idx_2d, (image_size, image_size))\n",
    "    p2d = np.stack((p2d_x, p2d_y), axis=-1)\n",
    "\n",
    "    l_pred = l_pred.view(batch_size, 3, -1)\n",
    "    \n",
    "    p3d_x = l_pred[:, 0].data.numpy()\n",
    "    p3d_x = p3d_x[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d_y = l_pred[:, 1].data.numpy()\n",
    "    p3d_y = p3d_y[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d_z = l_pred[:, 2].data.numpy()\n",
    "    p3d_z = p3d_z[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d = np.stack((p3d_x.reshape(batch_size, num_joints), p3d_y.reshape(batch_size, num_joints), p3d_z.reshape(batch_size, num_joints)), axis=-1)\n",
    "    \n",
    "    return p2d, p3d\n",
    "\n",
    "print(\"Evaluation...\")\n",
    "eval_loss = 0\n",
    "eval_iter = 0\n",
    "def eval_net():\n",
    "    for idx, batch in enumerate(loader_train):\n",
    "        eval_iter = idx\n",
    "        image = batch['image'].float()\n",
    "        pos2d_list = batch['pos_2d'] # size (N, 21, 2)\n",
    "        pos3d_list = batch['pos_3d'] # size (N, 21, 3)\n",
    "        loc_map, heatmap, one_hot = makeMaps(pos2d_list, pos3d_list)\n",
    "        y_pred = model(image)\n",
    "        h_pred = modelHeatmap(y_pred)\n",
    "        l_pred = modelLocmap(y_pred)\n",
    "        #print(\"output shape: {}\".format(y_pred.shape))\n",
    "        # use heatmap loss defined in VNect\n",
    "        #loss = computeLoss(heatmap, one_hot, loc_map, h_pred, l_pred)\n",
    "\n",
    "        #eval_loss = eval_loss + loss\n",
    "\n",
    "        if idx == 0:\n",
    "            # show some images\n",
    "            p2d, p3d = makePosList(h_pred[0], l_pred[0])\n",
    "            show_joints(image[0].data.numpy().transpose((1,2,0)), p2d, p3d)\n",
    "            show_joints(image[0].data.numpy().transpose((1,2,0)), batch['pos_2d'][0], batch['pos_3d'][0])\n",
    "            break\n",
    "#eval_net()\n",
    "#eval_iter = eval_iter + 1\n",
    "#print(\"Eval Loss: {}\".format(eval_loss / eval_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fully Connected Layer to Predict Gesture**\n",
    "\n",
    "Input: The outputs of our joint prediction model outputs:\n",
    "\n",
    "p2d: # size (N, 21, 2)\n",
    "p3d: # size (N, 21, 3)\n",
    "\n",
    "Output:\n",
    "\n",
    "y: # size (N, C=10), where C is the number of gesture classes\n",
    "\n",
    "**Note:** We will use both the 2D positions and the 3D positions of the joints to figure out what the gesture is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the gesture data\n",
    "g_dataset = GestureDataset('gesture_dataset.csv', transform=transform, train=True)\n",
    "\n",
    "N = len(g_dataset)\n",
    "\n",
    "loader_g_train = DataLoader(g_dataset, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8))))\n",
    "\n",
    "loader_g_val = DataLoader(g_dataset, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8), int(N*0.9))))\n",
    "\n",
    "loader_g_test = DataLoader(g_dataset, batch_size=batch_size,\n",
    "                         sampler=sampler.SubsetRandomSampler(range(int(N*0.9),N)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our fc should just output probabilities\n",
    "fc = nn.Sequential(\n",
    "    nn.Linear(in_features=105, out_features=105),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=105, out_features=50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=50, out_features=10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# predicted gesture probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the FC loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines constants\n",
    "batch_idx = torch.from_numpy(np.arange(batch_size)).long()\n",
    "\n",
    "def compute_g_loss(p2d, p3d, g_GT):\n",
    "    p2d = torch.from_numpy(p2d)\n",
    "    p3d = torch.from_numpy(p3d)\n",
    "\n",
    "    # put 2D and 3D joint positions together\n",
    "    fc_in = torch.cat((p2d.float(), p3d), dim=2) # shape (N, 21, 5)\n",
    "    fc_in = fc_in.view(batch_size, -1) # shape(N, 105)\n",
    "\n",
    "    # g_GT is of size (N, ) it just contains the labels for the batch\n",
    "    g_pred = fc(fc_in) # shape (N, 10) \n",
    "    g_loss = torch.sum(-1.0 * (g_pred[batch_idx, g_GT.long()] + epsilon).log()) / batch_size\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freeze Joint Prediction Network and Define Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# completely free out JP Net# comple \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in modelHeatmap.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in modelLocmap.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimize fully connected network only\n",
    "g_optimizer = torch.optim.Adam(fc.parameters(), lr=1.0e-4)\n",
    "g_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    for idx, batch in enumerate(loader_g_train):\n",
    "        # make all the ground truth tensors needed for loss computation\n",
    "        image = batch['image'].float()\n",
    "        # y_pred is of size 64 x 224 x 224\n",
    "        y_pred = model(image)\n",
    "        \n",
    "        # h_pred is of size 21 x 224 x 224\n",
    "        h_pred = modelHeatmap(y_pred)\n",
    "        \n",
    "        # l_pred is of size 3 x 224 x 224, the 3 representing x, y, z location maps of all 21 joints\n",
    "        l_pred = modelLocmap(y_pred)\n",
    "        \n",
    "        p2d, p3d = makePosListBatch(h_pred, l_pred)\n",
    "        print(p2d.shape, p3d.shape)\n",
    "        # print and store the loss curve\n",
    "        \n",
    "        g_loss = compute_g_loss(p2d, p3d, batch['label'])\n",
    "        \n",
    "        print(\"G Loss: {}\".format(g_loss))\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        \n",
    "        g_optimizer.step()\n",
    "        # Clears the gradients of all optimized torch.Tensor s\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "    torch.save(fc.state_dict(), 'fc_param.pt')\n",
    "    torch.save(g_optimizer.state_dict(), 'g_optimizer_param.pt')\n",
    "    torch.save({'losses': g_losses, 'epoch': epoch + 1}, 'g_training_param.pt')\n",
    "        \n",
    "print(\"======Training Done======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
