{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resouces:**\n",
    "\n",
    "Explanation on the resnet architecture: input_size/output_size/kernel/stride at each layer:\n",
    "https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624\n",
    "\n",
    "Resnet50 Architecture:\n",
    "https://www.kaggle.com/keras/resnet50\n",
    "\n",
    "Simple way of unpacking resnetX for stripping out FC layers and such:\n",
    "https://discuss.pytorch.org/t/resnet-pretrained-model-with-last-fc-layer-stripped-does-not-work/17951\n",
    "\n",
    "Reason as to why we want to resize each image and their labels to 224 x 224:\n",
    "https://stackoverflow.com/questions/43922308/what-input-image-size-is-correct-for-the-version-of-resnet-v2-in-tensorflow-slim\n",
    "\n",
    "How to modify the FC layer of resnet:\n",
    "https://discuss.pytorch.org/t/how-to-modify-the-final-fc-layer-based-on-the-torch-model/766/3\n",
    "\n",
    "How to partially freeze resnet34:\n",
    "https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing requirement:**\n",
    "\n",
    "In order to define the heatmap loss as torch.nn.functional.cross_entropy(input, target, weight=None, size_average=True, ignore_index=-100, reduce=True):\n",
    "\n",
    "We need to have the target/label take on the form of (N, J, H, W)\n",
    "\n",
    "Each j in J represents a joint\n",
    "\n",
    "**Important:** The image is of size (N, 3, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our own functions\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from libs.data_utils import HandDataset, ToTensor, Scale, GestureDataset\n",
    "from libs.layer_utils import flatten, random_weight, zero_weight\n",
    "from torchvision import transforms, utils\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# define some constants\n",
    "batch_size = 4\n",
    "num_joints = 21\n",
    "image_size = 224\n",
    "dtype = torch.float32\n",
    "epsilon = 1e-8\n",
    "transform = transforms.Compose([\n",
    "    Scale(image_size, image_size),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "def printGradientMaxMin(grad, name):\n",
    "    print(name, \"'s grad => max value: \", torch.max(grad), \" min value: \", torch.min(grad));\n",
    "\n",
    "def show_joints(image, pos_2d, pos_3d):\n",
    "    fig = plt.figure(figsize=plt.figaspect(2.))\n",
    "    ax = fig.add_subplot(2,1,1)\n",
    "    height, width, depth = image.shape\n",
    "    ax.imshow(image)\n",
    "    ax.scatter(pos_2d[:,0], pos_2d[:, 1], s=10, marker='.', c='r')\n",
    "    ax = fig.add_subplot(2,1,2, projection=\"3d\")\n",
    "    ax.view_init(-90,-90)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.scatter(pos_3d[:,0], pos_3d[:,1], pos_3d[:,2], s=30)\n",
    "    plt.show()\n",
    "\n",
    "# define heatmap guassian size\n",
    "g_heatmap_size = 9\n",
    "b_idx = torch.from_numpy(np.repeat(np.arange(batch_size), num_joints)).long()\n",
    "j_idx = torch.from_numpy( np.array(list(np.arange(num_joints))*batch_size) ).long()\n",
    "\n",
    "def makeHeatMapOneHot(pos2d_list):\n",
    "    # heatmap 0 based, should we have a very small number instead of 0?\n",
    "    one_hot = torch.zeros(batch_size, num_joints, image_size, image_size) # size (N, 21, 224, 224)\n",
    "    heatmap = torch.zeros(batch_size, num_joints, image_size + g_heatmap_size - 1, image_size + g_heatmap_size - 1) \n",
    "    # size (N, 21, 224 + 8, 224 + 8)\n",
    "    # Hao: Took me a while, but eventually figured out a way to do 4D indexing\n",
    "    \n",
    "    h_idx = pos2d_list[:, :, 1].view(-1).long()\n",
    "    w_idx = pos2d_list[:, :, 0].view(-1).long()\n",
    "    \n",
    "    one_hot[b_idx, j_idx, h_idx, w_idx] = 1.0\n",
    "    \n",
    "    padding = int((g_heatmap_size - 1)/2)\n",
    "    \n",
    "    # still need some loops, but at least we only need a double loop, yeahhh, I guess\n",
    "    for dh in range(-4, 5):\n",
    "        for dw in range(-4, 5):\n",
    "            cur_h_idx = h_idx + dh + padding\n",
    "            cur_w_idx = w_idx + dw + padding\n",
    "            cur_h_idx = cur_h_idx.long()\n",
    "            cur_w_idx = cur_w_idx.long()\n",
    "            heatmap[b_idx, j_idx, cur_h_idx, cur_w_idx] = math.exp(-1.0 * (dw**2 + dh**2))\n",
    "            #loc_map_x[b_idx, j_idx, cur_h_idx, cur_w_idx] = pos3d_list[:, :, 0].view(-1).float()\n",
    "            #loc_map_y[b_idx, j_idx, cur_h_idx, cur_w_idx] = pos3d_list[:, :, 1].view(-1).float()\n",
    "            #loc_map_z[b_idx, j_idx, cur_h_idx, cur_w_idx] = pos3d_list[:, :, 2].view(-1).float()\n",
    "            \n",
    "    heatmap = heatmap[:, :, padding:-padding, padding:-padding]\n",
    "    \n",
    "    return heatmap, one_hot\n",
    "    \n",
    "def makeMaps(pos2d_list, pos3d_list):\n",
    "    \n",
    "    # this is a quadruple loop for now, can we possibly verctorize it?\n",
    "    # Change: make locmaps take on constant value through out spatial dimension\n",
    "    # for example, loc_map_x(b=0, j=0, h=:, w=:) should take on the x value of joint 0 in sample 0\n",
    "    loc_map_x = torch.zeros(batch_size, num_joints, image_size, image_size)\n",
    "    loc_map_y = torch.zeros(batch_size, num_joints, image_size, image_size)\n",
    "    loc_map_z = torch.zeros(batch_size, num_joints, image_size, image_size)\n",
    "    \n",
    "    loc_map_x += pos3d_list[:, :, 0].view(batch_size, num_joints, 1, 1).float()\n",
    "    loc_map_y += pos3d_list[:, :, 1].view(batch_size, num_joints, 1, 1).float()\n",
    "    loc_map_z += pos3d_list[:, :, 2].view(batch_size, num_joints, 1, 1).float()\n",
    "    \n",
    "    heatmap, one_hot = makeHeatMapOneHot(pos2d_list)       \n",
    "\n",
    "    #loc_map = torch.cat((loc_map_x[:, :, padding:-padding, padding:-padding], loc_map_y[:, :, padding:-padding, padding:-padding], loc_map_z[:, :, padding:-padding, padding:-padding]), dim=1)\n",
    "    loc_map = torch.cat((loc_map_x, loc_map_y, loc_map_z), dim=1)\n",
    "    return loc_map, heatmap, one_hot\n",
    "\n",
    "def makeMapsNaive(pos2d_list, pos3d_list):\n",
    "    \n",
    "    # heatmap 0 based, should we have a very small number instead of 0\n",
    "    heatmap = torch.zeros(batch_size, num_joints, image_size, image_size) # size (N, 21, 224, 224)\n",
    "    one_hot = torch.zeros(batch_size, num_joints, image_size, image_size) # size (N, 21, 224, 224)\n",
    "    # this is a quadruple loop for now, can we possibly verctorize it?\n",
    "    loc_map_x = torch.zeros(batch_size, num_joints, image_size, image_size)\n",
    "    loc_map_y = torch.zeros(batch_size, num_joints, image_size, image_size)\n",
    "    loc_map_z = torch.zeros(batch_size, num_joints, image_size, image_size)\n",
    "    \n",
    "    loc_map_x = loc_map_x + pos3d_list[:, :, 0].view(batch_size, num_joints, 1, 1).float()\n",
    "    loc_map_y = loc_map_y + pos3d_list[:, :, 1].view(batch_size, num_joints, 1, 1).float()\n",
    "    loc_map_z = loc_map_z + pos3d_list[:, :, 2].view(batch_size, num_joints, 1, 1).float()\n",
    "    \n",
    "    for b in range(0, batch_size):\n",
    "        for i in range(0, num_joints):\n",
    "            one_hot[b, i, int(pos2d_list[b, i, 1]), int(pos2d_list[b, i, 0])] = 1.0\n",
    "            \n",
    "            for delta_x in range(-4, 5):\n",
    "                x = pos2d_list[b, i, 0] + delta_x\n",
    "                if x > 0 and x < image_size:\n",
    "                    for delta_y in range(-4, 5):\n",
    "                        y = pos2d_list[b, i, 1] + delta_y\n",
    "                        if y > 0 and y < image_size:\n",
    "                            # guassian-like heatmap\n",
    "                            heatmap[b, i, int(y), int(x)] = math.exp(-1.0 * (delta_x**2 + delta_y**2))\n",
    "                            #1.0 - (delta_x**2 + delta_y**2) / 32\n",
    "                            # all points around the joint take on the 3D position of the joint for our location map GT\n",
    "                            #loc_map_x[b, i, int(y), int(x)] = pos3d_list[b, i, 0].float()\n",
    "                            #loc_map_y[b, i, int(y), int(x)] = pos3d_list[b, i, 1].float()\n",
    "                            #loc_map_z[b, i, int(y), int(x)] = pos3d_list[b, i, 2].float()\n",
    "\n",
    "    loc_map = torch.cat((loc_map_x, loc_map_y, loc_map_z), dim=1)\n",
    "    \n",
    "    return loc_map, heatmap, one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 3, 224, 224])\n",
      "0 torch.Size([4, 21, 2])\n",
      "0 torch.Size([4, 21, 3])\n",
      "tensor([ 62.1957, -33.8723,   2.6454], dtype=torch.float64)\n",
      "loc_map torch.Size([4, 63, 224, 224])\n",
      "heatmap torch.Size([4, 21, 224, 224])\n",
      "one_hot torch.Size([4, 21, 224, 224])\n",
      "vectorized: --- 0.05983901023864746 seconds ---\n",
      "naive: --- 0.20545196533203125 seconds ---\n",
      "Check the difference between vectorized impl and naive impl:\n",
      "Error: tensor(0.)\n",
      "Error: tensor(0.)\n",
      "Error: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "hand_train = HandDataset('toy_dataset.csv', transform=transform, train=True)\n",
    "N = 20 #len(hand_train)\n",
    "loader_train = DataLoader(hand_train, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8))))\n",
    "\n",
    "hand_val = HandDataset('toy_dataset.csv', transform=transform, train=True)\n",
    "loader_val = DataLoader(hand_val, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8), int(N*0.9))))\n",
    "\n",
    "hand_test = HandDataset('toy_dataset.csv', transform=transform, train=False)\n",
    "loader_test = DataLoader(hand_test, batch_size=batch_size,\n",
    "                         sampler=sampler.SubsetRandomSampler(range(int(N*0.9),N)))\n",
    "\n",
    "for i_batch, batch in enumerate(loader_train):\n",
    "    print(i_batch, batch['image'].size())\n",
    "    print(i_batch, batch['pos_2d'].size())\n",
    "    print(i_batch, batch['pos_3d'].size())\n",
    "    image = batch['image']\n",
    "    pos2d_list = batch['pos_2d'] # size (N, 21, 2)\n",
    "    pos3d_list = batch['pos_3d'] # size (N, 21, 3)\n",
    "    print(pos3d_list[0, 0])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    loc_map, heatmap, one_hot = makeMaps(pos2d_list, pos3d_list)\n",
    "    print(\"loc_map\", loc_map.size())\n",
    "    print(\"heatmap\", heatmap.size())\n",
    "    print(\"one_hot\", one_hot.size())\n",
    "    print(\"vectorized: --- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    loc_map_n, heatmap_n, one_hot_n = makeMapsNaive(pos2d_list, pos3d_list)\n",
    "    print(\"naive: --- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    print(\"Check the difference between vectorized impl and naive impl:\")\n",
    "    print(\"Error:\", torch.sum(loc_map - loc_map_n))\n",
    "    print(\"Error:\", torch.sum(heatmap - heatmap_n))\n",
    "    print(\"Error:\", torch.sum(one_hot - one_hot_n))\n",
    "    \n",
    "    #print(heatmap.shape)\n",
    "    #print(loc_map[0, 0])\n",
    "    #for j in range(image_size):\n",
    "    #    print(heatmap[0, 0, j])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the model:**\n",
    "\n",
    "1) Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect a value:  tensor(-0.3724)\n",
      "Error:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# bone length bias, this is a learnable parameter!\n",
    "# blb = torch.zeros(22, dtype=torch.float, requires_grad=True)\n",
    "# bone length weights\n",
    "blw = torch.zeros(22, 21, dtype=torch.float)\n",
    "bone_list=[(16, 18), (18, 19), (1, 0), (0, 2), (2, 3), (5, 4), (4, 6), \n",
    "           (6, 7), (13, 12), (12, 14), (14, 15), (9, 8), (8, 10), (10, 11), \n",
    "           (17, 16), (17, 1), (17, 5), (17, 13), (17, 9), (1, 5), (5, 13), (13, 9)]\n",
    "\n",
    "for idx, b in enumerate(bone_list):\n",
    "    blw[idx, b[0]] = 1.0\n",
    "    blw[idx, b[1]] = -1.0\n",
    "\n",
    "#print(blw)\n",
    "\n",
    "blw = blw.transpose(0, 1)\n",
    "loss_scale = 1.0 / (batch_size * num_joints * g_heatmap_size**2 * 3)\n",
    "\n",
    "testM = torch.randn(4, 21)\n",
    "\n",
    "pres = torch.matmul(testM, blw)\n",
    "\n",
    "print(\"Inspect a value: \", pres[0, 0])\n",
    "print(\"Error: \", pres[0, 0] - (testM[0, 16] - testM[0, 18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use renet 34 for speed, later may use resnet50 as our base NN stucture for joints detection\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "#resnet34.cuda()\n",
    "\n",
    "# resnet 34 has 0-9, which is 10 immediate children modules, we are discarding the last 2, so 0-7\n",
    "# and let's only freeze 0-4\n",
    "\n",
    "'''\n",
    "# Here we are freezing all layers of resnet 34, which probably isn't a great idea, we know that pretrained resnet learns\n",
    "# a lot of information irrelevant to our task, such as what kind of dogs are in the images\n",
    "for param in resnet34.parameters():\n",
    "    # don't change/update the pretrained model parameters, only change the final fc layer\n",
    "    param.requires_grad = False\n",
    "'''\n",
    "def freezeResnet():\n",
    "    for num, child in enumerate(resnet34.children()):\n",
    "        # freeze the lower layers only\n",
    "        #print(num)\n",
    "        if num < 4:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "freezeResnet()\n",
    "# Upsample using transpose convolution and unpooling\n",
    "# output_padding = 1 is intended to recover proper size\n",
    "transConv1 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "transConv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "transConv3 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "# Note: the following layer should have been maxunpooling, kernel 3 x 3, stride 2\n",
    "# torch.nn.MaxUnpool2d() requires modification of the torchvision resnetXX modules\n",
    "# to have the maxpooling layer return indices of max values, try ConvTranspose2d() instead\n",
    "# If this doesn't work, have to and implement custom version of resnet\n",
    "transConv4 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "transConv5 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=7, stride=2, padding=3, output_padding=1) # (224 x 224)\n",
    "\n",
    "# layers to generate the heatmap\n",
    "#conv6h = nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# finally, the joint prediction Convolutional Layer, filter size 3 x 3, 21 filters\n",
    "# VNect's 2D heatmap is generated from res4d, and location maps from res5a, we try something simpler\n",
    "jointPrediction = nn.Conv2d(in_channels=128, out_channels=21, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# layers to generate the location maps\n",
    "conv6 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "conv7 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# what if we try to squash 21 heatmaps into one map, why would we need to predict a location map for each joint?\n",
    "# We should predict a location map for each of x, y, z once per image\n",
    "conv8 = nn.Conv2d(in_channels=32, out_channels=8, kernel_size=1, stride=1, padding=0)\n",
    "locationPrediction = nn.Conv2d(in_channels=8, out_channels=3, kernel_size=15, stride=1, padding=7)\n",
    "\n",
    "# stick a bunch of ReLu non-linearity in the upsampling pipeline\n",
    "model = nn.Sequential(\n",
    "    *list(resnet34.children())[:-3],\n",
    "    transConv2,\n",
    "    nn.ReLU(),\n",
    "    transConv3,\n",
    "    nn.ReLU(),\n",
    "    transConv4,\n",
    "    nn.ReLU(),\n",
    "    transConv5,\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "modelHeatmap = nn.Sequential(\n",
    "    jointPrediction\n",
    ")\n",
    "\n",
    "modelLocmap = nn.Sequential(\n",
    "    conv6,\n",
    "    nn.ReLU(),\n",
    "    conv7,\n",
    "    nn.ReLU(),\n",
    "    conv8,\n",
    "    nn.ReLU(),\n",
    "    locationPrediction\n",
    ")\n",
    "\n",
    "# use a toy model to make sure that the location map implementations are correct\n",
    "#model = nn.Sequential(\n",
    "#    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "#    nn.Conv2d(in_channels=64, out_channels=63, kernel_size=3, stride=1, padding=1)\n",
    "#)\n",
    "\n",
    "# model.cuda()\n",
    "# .cuda() makes model run on GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model and evaluate on eval dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer using adam\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, list(model.parameters()) + list(modelHeatmap.parameters()) + list(modelLocmap.parameters()) ), lr=1.0e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ComputeLoss(object):\n",
    "    def __init__(self, two_stage=True, stack_size=40):\n",
    "        self.h_losses = []\n",
    "        self.two_stage = two_stage\n",
    "        self.stack_size = stack_size\n",
    "        self.stage = 0\n",
    "        \n",
    "    def __call__(self, heatmap, one_hot, loc_map, hmap_pred, l_pred):\n",
    "        # TODO: how to combine l_loss and h_loss is about hyper-parameter tuning\n",
    "        global optimizer\n",
    "        global modelLocmap\n",
    "        #print(\"Softmax Error: \", torch.sum(torch.sum(hmap_pred, dim=2) - 1.0))\n",
    "        hmap_pred = nn.functional.softmax(h_pred.view(batch_size, num_joints, -1), dim=2)\n",
    "        h_loss = torch.sum(hmap_pred * one_hot.view(batch_size, num_joints, -1), dim=2) + epsilon\n",
    "        h_loss = torch.sum(-1.0 * h_loss.log()) / (batch_size * num_joints)\n",
    "        #torch.sum(torch.pow(nn.functional.softmax(y_pred[:, 63:, :, :].view(batch_size, 21, -1), dim=2) - heatmap.view(batch_size, 21, -1), 2)) / batch_size\n",
    "\n",
    "\n",
    "        #Hao: construct a combined heatmap of both the ground truth and our predicted heatmap\n",
    "        #since the fact that joints exist at a certain point should offer clue to what its 3D position might be\n",
    "\n",
    "\n",
    "        # j_idx = torch.argmax(hmap_pred.view(batch_size * num_joints, -1), dim=1)\n",
    "\n",
    "        # p2d_y_n, p2d_x_n = np.unravel_index(j_idx.data.numpy(), (image_size, image_size))\n",
    "        # p2d_y_n = p2d_y_n.reshape((batch_size, num_joints))\n",
    "        # p2d_x_n = p2d_x_n.reshape((batch_size, num_joints))\n",
    "        # p2d_n = np.stack((p2d_x_n, p2d_y_n), axis=-1)\n",
    "\n",
    "\n",
    "        j_idx_flat = torch.argmax(hmap_pred.view(batch_size, num_joints, -1), dim=2)\n",
    "        # because j_idx_flat is already a long tensor, the decimal truncation is implicit, no need to floor()\n",
    "        p2d_y = j_idx_flat / image_size\n",
    "        p2d_x = torch.remainder(j_idx_flat, image_size)\n",
    "        p2d = torch.cat((p2d_x.view(batch_size, num_joints, 1), p2d_y.view(batch_size, num_joints, 1)), dim=2)\n",
    "\n",
    "        # print(\"Error is: \", torch.sum(torch.from_numpy(p2d_n) - p2d))\n",
    "        # greater_than_max = h_pred[0, 0, :, :] >= h_pred[0, 0, p2d[0, 0, 1], p2d[0, 0, 0]]\n",
    "        # print(h_pred.size(), \" Error is: \", torch.sum(greater_than_max) - 1)\n",
    "\n",
    "        #hmap_pred.register_hook(lambda grad: printGradientMaxMin(grad, 'hmap_pred'))\n",
    "\n",
    "        hp, one_hot_pred = makeHeatMapOneHot(p2d)\n",
    "\n",
    "        # total heatmap\n",
    "        t_heatmap = hp + heatmap\n",
    "        t_heatmap = t_heatmap.repeat(1, 3, 1, 1) # shape(N, 63, 224, 224)\n",
    "\n",
    "        l_pred = torch.cat((l_pred[:, 0:1, :, :].expand(-1, 21, -1, -1), \n",
    "                            l_pred[:, 1:2, :, :].expand(-1, 21, -1, -1), \n",
    "                            l_pred[:, 2:3, :, :].expand(-1, 21, -1, -1)), dim=1)\n",
    "        l_loss = torch.sum(torch.pow(t_heatmap * (l_pred - loc_map), 2)) * loss_scale\n",
    "        #l_loss = l_loss**0.5\n",
    "\n",
    "        # Skeleton (bone length) constraint\n",
    "        # get a weighted average of predicted joint coordinates\n",
    "        #coord_p = torch.sum(torch.sum(t_heatmap * l_pred, dim=3), dim=2)\n",
    "        coord_p = torch.sum(torch.sum(one_hot_pred.repeat(1, 3, 1, 1) * l_pred, dim=3), dim=2)\n",
    "        x_p = coord_p[:, 0:21]\n",
    "        y_p = coord_p[:, 21:42]\n",
    "        z_p = coord_p[:, 42:63]\n",
    "\n",
    "        #x_p.register_hook(lambda grad: printGradientMaxMin(grad, 'x_p'))\n",
    "        #y_p.register_hook(lambda grad: printGradientMaxMin(grad, 'y_p'))\n",
    "        #z_p.register_hook(lambda grad: printGradientMaxMin(grad, 'z_p'))\n",
    "\n",
    "        # GT bone length\n",
    "        blb = (torch.matmul(loc_map[:, 0:num_joints, 0, 0], blw)**2 + torch.matmul(loc_map[:, num_joints:2*num_joints, 0, 0], blw)**2 + torch.matmul(loc_map[:, 2*num_joints:3*num_joints, 0, 0], blw)**2)\n",
    "\n",
    "        # predicted bone length, using predicted joint 2D location as filter, rather than GT 2D joint location\n",
    "        bl_x = torch.matmul(x_p, blw)**2\n",
    "        bl_y = torch.matmul(y_p, blw)**2\n",
    "        bl_z = torch.matmul(z_p, blw)**2\n",
    "\n",
    "        #bl_x.register_hook(lambda grad: printGradientMaxMin(grad, 'bl_x'))\n",
    "        #bl_y.register_hook(lambda grad: printGradientMaxMin(grad, 'bl_y'))\n",
    "        #bl_z.register_hook(lambda grad: printGradientMaxMin(grad, 'bl_z'))\n",
    "\n",
    "        # add epsilon for numerical stability, since sqrt(x)'s derivative is 0.5/sqrt(x)\n",
    "        bone_diff = torch.sqrt(bl_x + bl_y + bl_z + epsilon) - torch.sqrt(blb + epsilon)\n",
    "\n",
    "        #bone_diff.register_hook(lambda grad: printGradientMaxMin(grad, 'bone_diff'))\n",
    "\n",
    "        bl_loss = torch.sum(bone_diff.abs()) / (batch_size * num_joints)\n",
    "\n",
    "        #bl_loss.register_hook(lambda grad: printGradientMaxMin(grad, 'bl_loss'))\n",
    "\n",
    "        # first minimize the 2D prediction h_loss\n",
    "        # then focus on the 3D location loss\n",
    "        loss = 0.0\n",
    "        \n",
    "        if self.two_stage:\n",
    "            #print(self.two_stage, ' ', len(self.h_losses))\n",
    "            self.h_losses.append(h_loss.data.numpy())\n",
    "            # only maintain 10 past losses\n",
    "            if len(self.h_losses) > self.stack_size:\n",
    "                self.h_losses = self.h_losses[1:]\n",
    "            \n",
    "            if self.stage == 0 and (len(self.h_losses) < self.stack_size or np.mean(self.h_losses[0:int(self.stack_size/2)]) > np.mean(self.h_losses[int(self.stack_size/2):]) ):\n",
    "                #print(np.mean(self.h_losses) > self.h_losses[-1], ' ', np.mean(self.h_losses), ' ', self.h_losses[-1])\n",
    "                # if the first half mean is greater than the second half mean, h_loss still needs to be minimized\n",
    "                l_loss = 0.05 * l_loss\n",
    "                bl_loss = 0.05 * bl_loss\n",
    "            else:\n",
    "                # move on to stage 1, we won't go back to stage 0\n",
    "                # focus on 3D location loss\n",
    "                # freeze model basis portion and the 2D heatmap portion\n",
    "                if self.stage == 0:\n",
    "                    self.stage = 1\n",
    "                    print(\"###### Stage 1 Start (Second Stage) ######\")\n",
    "                    # only need to freeze the parameters once\n",
    "                    for param in model.parameters():\n",
    "                        param.requires_grad = False\n",
    "                    for param in modelHeatmap.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "                    optimizer = torch.optim.Adam(modelLocmap.parameters(), lr=1.0e-4)\n",
    "        \n",
    "        print(\"h_loss: \", h_loss, \" l_loss: \", l_loss, \" bl_loss: \", bl_loss)\n",
    "        loss = h_loss + l_loss + bl_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "# test loss function implementation\n",
    "\n",
    "# loc_map_t, heatmap_t, one_hot_t = makeMaps(torch.rand(batch_size, num_joints, 2)*223, torch.rand(batch_size, num_joints, 3)*223)\n",
    "# h_pred_t = torch.randn(batch_size, 21, image_size, image_size)\n",
    "# l_pred_t = torch.randn(batch_size, 3, image_size, image_size)\n",
    "# l = computeLoss(heatmap_t, one_hot_t, loc_map_t, h_pred_t, l_pred_t)\n",
    "# print(l)\n",
    "losses = []\n",
    "computeLoss = ComputeLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load trained models (ONLY Run When You Want to Resume Training)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load saved model and training parameters to resume training\n",
    "model.load_state_dict(torch.load('model_param.pt'))\n",
    "modelHeatmap.load_state_dict(torch.load('modelHeatmap_param.pt'))\n",
    "modelLocmap.load_state_dict(torch.load('modelLocmap_param.pt'))\n",
    "#optimizer.load_state_dict(torch.load('optimizer_param.pt'))\n",
    "#training_param = torch.load('training_param.pt')\n",
    "#computeLoss = training_param['computeLoss']\n",
    "#losses = training_param['losses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "h_loss:  tensor(10.8226)  l_loss:  tensor(9.2056)  bl_loss:  tensor(1.9600)\n",
      "Loss: 21.988239288330078\n",
      "h_loss:  tensor(10.8230)  l_loss:  tensor(9.5295)  bl_loss:  tensor(1.9606)\n",
      "Loss: 22.3131160736084\n",
      "h_loss:  tensor(10.8171)  l_loss:  tensor(9.4727)  bl_loss:  tensor(1.9645)\n",
      "Loss: 22.25424575805664\n",
      "h_loss:  tensor(10.8156)  l_loss:  tensor(9.5568)  bl_loss:  tensor(1.9653)\n",
      "Loss: 22.337682723999023\n",
      "Epoch: 1\n",
      "h_loss:  tensor(10.7686)  l_loss:  tensor(9.4563)  bl_loss:  tensor(1.9651)\n",
      "Loss: 22.189987182617188\n",
      "h_loss:  tensor(10.6205)  l_loss:  tensor(9.4726)  bl_loss:  tensor(1.9639)\n",
      "Loss: 22.057008743286133\n",
      "h_loss:  tensor(10.0631)  l_loss:  tensor(9.2015)  bl_loss:  tensor(1.9580)\n",
      "Loss: 21.222625732421875\n",
      "h_loss:  tensor(10.1862)  l_loss:  tensor(9.0963)  bl_loss:  tensor(1.9392)\n",
      "Loss: 21.221567153930664\n",
      "Epoch: 2\n",
      "h_loss:  tensor(9.5373)  l_loss:  tensor(8.6505)  bl_loss:  tensor(1.9359)\n",
      "Loss: 20.123584747314453\n",
      "h_loss:  tensor(8.6517)  l_loss:  tensor(7.6695)  bl_loss:  tensor(1.9300)\n",
      "Loss: 18.25113868713379\n",
      "h_loss:  tensor(9.0070)  l_loss:  tensor(10.5333)  bl_loss:  tensor(1.8930)\n",
      "Loss: 21.433286666870117\n",
      "h_loss:  tensor(8.7019)  l_loss:  tensor(7.8607)  bl_loss:  tensor(1.9105)\n",
      "Loss: 18.47307586669922\n",
      "Epoch: 3\n",
      "h_loss:  tensor(9.0439)  l_loss:  tensor(8.7184)  bl_loss:  tensor(1.9069)\n",
      "Loss: 19.66925048828125\n",
      "h_loss:  tensor(7.9286)  l_loss:  tensor(7.1806)  bl_loss:  tensor(1.8822)\n",
      "Loss: 16.99142837524414\n",
      "h_loss:  tensor(7.9616)  l_loss:  tensor(7.8575)  bl_loss:  tensor(1.9207)\n",
      "Loss: 17.739797592163086\n",
      "h_loss:  tensor(7.7324)  l_loss:  tensor(8.1240)  bl_loss:  tensor(1.8656)\n",
      "Loss: 17.72208595275879\n",
      "Epoch: 4\n",
      "h_loss:  tensor(7.6935)  l_loss:  tensor(7.4790)  bl_loss:  tensor(1.9261)\n",
      "Loss: 17.09862518310547\n",
      "h_loss:  tensor(7.9311)  l_loss:  tensor(8.3373)  bl_loss:  tensor(1.9324)\n",
      "Loss: 18.200843811035156\n",
      "h_loss:  tensor(7.4296)  l_loss:  tensor(8.5405)  bl_loss:  tensor(1.9244)\n",
      "Loss: 17.89437484741211\n",
      "h_loss:  tensor(7.6408)  l_loss:  tensor(6.0136)  bl_loss:  tensor(1.8860)\n",
      "Loss: 15.540449142456055\n",
      "Epoch: 5\n",
      "h_loss:  tensor(6.2591)  l_loss:  tensor(6.9423)  bl_loss:  tensor(1.8745)\n",
      "Loss: 15.075972557067871\n",
      "h_loss:  tensor(7.1899)  l_loss:  tensor(8.1744)  bl_loss:  tensor(1.8928)\n",
      "Loss: 17.257104873657227\n",
      "h_loss:  tensor(7.0834)  l_loss:  tensor(7.1456)  bl_loss:  tensor(1.8908)\n",
      "Loss: 16.11977767944336\n",
      "h_loss:  tensor(7.4222)  l_loss:  tensor(8.4455)  bl_loss:  tensor(1.9119)\n",
      "Loss: 17.779685974121094\n",
      "Epoch: 6\n",
      "h_loss:  tensor(6.7345)  l_loss:  tensor(6.9189)  bl_loss:  tensor(1.9118)\n",
      "Loss: 15.565225601196289\n",
      "h_loss:  tensor(6.3814)  l_loss:  tensor(6.7540)  bl_loss:  tensor(1.8976)\n",
      "Loss: 15.032962799072266\n",
      "h_loss:  tensor(7.5795)  l_loss:  tensor(8.4113)  bl_loss:  tensor(1.8781)\n",
      "Loss: 17.868879318237305\n",
      "h_loss:  tensor(7.4328)  l_loss:  tensor(7.7979)  bl_loss:  tensor(1.8886)\n",
      "Loss: 17.119295120239258\n",
      "Epoch: 7\n",
      "h_loss:  tensor(6.0569)  l_loss:  tensor(7.6980)  bl_loss:  tensor(1.8557)\n",
      "Loss: 15.610625267028809\n",
      "h_loss:  tensor(7.1279)  l_loss:  tensor(8.3814)  bl_loss:  tensor(1.8907)\n",
      "Loss: 17.400083541870117\n",
      "h_loss:  tensor(7.0417)  l_loss:  tensor(7.1991)  bl_loss:  tensor(1.8849)\n",
      "Loss: 16.125656127929688\n",
      "h_loss:  tensor(6.0723)  l_loss:  tensor(6.4170)  bl_loss:  tensor(1.8497)\n",
      "Loss: 14.339033126831055\n",
      "Epoch: 8\n",
      "h_loss:  tensor(5.5475)  l_loss:  tensor(4.9035)  bl_loss:  tensor(1.7468)\n",
      "Loss: 12.197882652282715\n",
      "h_loss:  tensor(6.2556)  l_loss:  tensor(6.6585)  bl_loss:  tensor(1.6302)\n",
      "Loss: 14.544295310974121\n",
      "h_loss:  tensor(7.1532)  l_loss:  tensor(11.8963)  bl_loss:  tensor(1.7396)\n",
      "Loss: 20.789024353027344\n",
      "h_loss:  tensor(7.0364)  l_loss:  tensor(8.4711)  bl_loss:  tensor(1.7690)\n",
      "Loss: 17.276416778564453\n",
      "Epoch: 9\n",
      "h_loss:  tensor(5.7200)  l_loss:  tensor(6.5639)  bl_loss:  tensor(1.7321)\n",
      "Loss: 14.015961647033691\n",
      "h_loss:  tensor(7.2770)  l_loss:  tensor(8.8049)  bl_loss:  tensor(1.7672)\n",
      "Loss: 17.849105834960938\n",
      "h_loss:  tensor(6.7487)  l_loss:  tensor(8.9342)  bl_loss:  tensor(1.7098)\n",
      "Loss: 17.392642974853516\n",
      "h_loss:  tensor(5.8666)  l_loss:  tensor(8.0071)  bl_loss:  tensor(1.6158)\n",
      "Loss: 15.489550590515137\n",
      "Epoch: 10\n",
      "h_loss:  tensor(5.9823)  l_loss:  tensor(7.4074)  bl_loss:  tensor(1.5285)\n",
      "Loss: 14.918198585510254\n",
      "h_loss:  tensor(5.8785)  l_loss:  tensor(7.5974)  bl_loss:  tensor(1.5291)\n",
      "Loss: 15.005051612854004\n",
      "h_loss:  tensor(5.8922)  l_loss:  tensor(6.6897)  bl_loss:  tensor(1.3847)\n",
      "Loss: 13.966547966003418\n",
      "h_loss:  tensor(6.4373)  l_loss:  tensor(9.6302)  bl_loss:  tensor(1.4581)\n",
      "Loss: 17.525619506835938\n",
      "Epoch: 11\n",
      "h_loss:  tensor(4.7698)  l_loss:  tensor(5.2447)  bl_loss:  tensor(1.3672)\n",
      "Loss: 11.38172721862793\n",
      "h_loss:  tensor(5.6050)  l_loss:  tensor(7.8244)  bl_loss:  tensor(1.5164)\n",
      "Loss: 14.945764541625977\n",
      "h_loss:  tensor(4.7401)  l_loss:  tensor(8.7179)  bl_loss:  tensor(1.4002)\n",
      "Loss: 14.858201026916504\n",
      "h_loss:  tensor(6.5223)  l_loss:  tensor(9.1681)  bl_loss:  tensor(1.4726)\n",
      "Loss: 17.163043975830078\n",
      "Epoch: 12\n",
      "h_loss:  tensor(5.1184)  l_loss:  tensor(6.9629)  bl_loss:  tensor(1.4843)\n",
      "Loss: 13.565637588500977\n",
      "h_loss:  tensor(5.3183)  l_loss:  tensor(8.6602)  bl_loss:  tensor(1.5409)\n",
      "Loss: 15.5194091796875\n",
      "h_loss:  tensor(6.1916)  l_loss:  tensor(9.0463)  bl_loss:  tensor(1.5366)\n",
      "Loss: 16.77446746826172\n",
      "h_loss:  tensor(5.4796)  l_loss:  tensor(6.5854)  bl_loss:  tensor(1.3913)\n",
      "Loss: 13.45628547668457\n",
      "Epoch: 13\n",
      "h_loss:  tensor(5.1506)  l_loss:  tensor(8.6787)  bl_loss:  tensor(1.4499)\n",
      "Loss: 15.279182434082031\n",
      "h_loss:  tensor(5.3814)  l_loss:  tensor(6.6748)  bl_loss:  tensor(1.2757)\n",
      "Loss: 13.331899642944336\n",
      "h_loss:  tensor(5.3326)  l_loss:  tensor(6.9085)  bl_loss:  tensor(1.3056)\n",
      "Loss: 13.546619415283203\n",
      "h_loss:  tensor(5.3823)  l_loss:  tensor(8.8011)  bl_loss:  tensor(1.3411)\n",
      "Loss: 15.524513244628906\n",
      "Epoch: 14\n",
      "h_loss:  tensor(4.8784)  l_loss:  tensor(8.3529)  bl_loss:  tensor(1.3491)\n",
      "Loss: 14.5804443359375\n",
      "h_loss:  tensor(5.2694)  l_loss:  tensor(8.7602)  bl_loss:  tensor(1.4477)\n",
      "Loss: 15.477313995361328\n",
      "h_loss:  tensor(4.9732)  l_loss:  tensor(7.7316)  bl_loss:  tensor(1.3686)\n",
      "Loss: 14.073378562927246\n",
      "h_loss:  tensor(4.7602)  l_loss:  tensor(7.5160)  bl_loss:  tensor(1.3784)\n",
      "Loss: 13.654548645019531\n",
      "Epoch: 15\n",
      "h_loss:  tensor(4.4872)  l_loss:  tensor(8.5662)  bl_loss:  tensor(1.3723)\n",
      "Loss: 14.425697326660156\n",
      "h_loss:  tensor(4.7980)  l_loss:  tensor(6.8844)  bl_loss:  tensor(1.2411)\n",
      "Loss: 12.923564910888672\n",
      "h_loss:  tensor(4.6758)  l_loss:  tensor(7.5871)  bl_loss:  tensor(1.1682)\n",
      "Loss: 13.431109428405762\n",
      "h_loss:  tensor(5.0762)  l_loss:  tensor(8.9478)  bl_loss:  tensor(1.3635)\n",
      "Loss: 15.387474060058594\n",
      "Epoch: 16\n",
      "h_loss:  tensor(4.7489)  l_loss:  tensor(9.0739)  bl_loss:  tensor(1.2060)\n",
      "Loss: 15.028820991516113\n",
      "h_loss:  tensor(5.0133)  l_loss:  tensor(7.4144)  bl_loss:  tensor(1.2224)\n",
      "Loss: 13.650127410888672\n",
      "h_loss:  tensor(4.1858)  l_loss:  tensor(7.4834)  bl_loss:  tensor(1.2815)\n",
      "Loss: 12.95065975189209\n",
      "h_loss:  tensor(4.7652)  l_loss:  tensor(7.6679)  bl_loss:  tensor(1.2174)\n",
      "Loss: 13.650484085083008\n",
      "Epoch: 17\n",
      "h_loss:  tensor(4.4297)  l_loss:  tensor(8.5405)  bl_loss:  tensor(1.3041)\n",
      "Loss: 14.27420711517334\n",
      "h_loss:  tensor(5.1178)  l_loss:  tensor(8.1445)  bl_loss:  tensor(1.3929)\n",
      "Loss: 14.655242919921875\n",
      "h_loss:  tensor(4.5135)  l_loss:  tensor(7.6312)  bl_loss:  tensor(1.2008)\n",
      "Loss: 13.345473289489746\n",
      "h_loss:  tensor(4.8849)  l_loss:  tensor(7.3822)  bl_loss:  tensor(1.2708)\n",
      "Loss: 13.537894248962402\n",
      "Epoch: 18\n",
      "h_loss:  tensor(4.7783)  l_loss:  tensor(8.8217)  bl_loss:  tensor(1.3644)\n",
      "Loss: 14.96441650390625\n",
      "h_loss:  tensor(3.9512)  l_loss:  tensor(6.5477)  bl_loss:  tensor(1.1084)\n",
      "Loss: 11.607231140136719\n",
      "h_loss:  tensor(5.0868)  l_loss:  tensor(7.3944)  bl_loss:  tensor(1.1411)\n",
      "Loss: 13.622343063354492\n",
      "h_loss:  tensor(4.9123)  l_loss:  tensor(9.0292)  bl_loss:  tensor(1.1904)\n",
      "Loss: 15.131864547729492\n",
      "Epoch: 19\n",
      "h_loss:  tensor(4.1432)  l_loss:  tensor(7.4583)  bl_loss:  tensor(1.2585)\n",
      "Loss: 12.859963417053223\n",
      "h_loss:  tensor(4.6215)  l_loss:  tensor(7.9595)  bl_loss:  tensor(1.4069)\n",
      "Loss: 13.987852096557617\n",
      "h_loss:  tensor(5.0084)  l_loss:  tensor(11.5631)  bl_loss:  tensor(1.2748)\n",
      "Loss: 17.84630584716797\n",
      "h_loss:  tensor(4.4121)  l_loss:  tensor(6.5509)  bl_loss:  tensor(1.2703)\n",
      "Loss: 12.233304977416992\n",
      "Epoch: 20\n",
      "h_loss:  tensor(4.6420)  l_loss:  tensor(6.9809)  bl_loss:  tensor(1.3604)\n",
      "Loss: 12.98338794708252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_loss:  tensor(4.7567)  l_loss:  tensor(7.6333)  bl_loss:  tensor(1.3769)\n",
      "Loss: 13.766899108886719\n",
      "h_loss:  tensor(4.3451)  l_loss:  tensor(7.2297)  bl_loss:  tensor(1.3295)\n",
      "Loss: 12.904297828674316\n",
      "h_loss:  tensor(6.9283)  l_loss:  tensor(11.6603)  bl_loss:  tensor(1.2679)\n",
      "Loss: 19.856534957885742\n",
      "Epoch: 21\n",
      "h_loss:  tensor(4.0449)  l_loss:  tensor(5.3893)  bl_loss:  tensor(1.1570)\n",
      "Loss: 10.591156959533691\n",
      "h_loss:  tensor(4.9424)  l_loss:  tensor(8.1769)  bl_loss:  tensor(1.3742)\n",
      "Loss: 14.493461608886719\n",
      "h_loss:  tensor(5.2808)  l_loss:  tensor(11.6930)  bl_loss:  tensor(1.2097)\n",
      "Loss: 18.183551788330078\n",
      "h_loss:  tensor(4.3538)  l_loss:  tensor(7.1907)  bl_loss:  tensor(1.2853)\n",
      "Loss: 12.82979965209961\n",
      "Epoch: 22\n",
      "h_loss:  tensor(5.3036)  l_loss:  tensor(7.7921)  bl_loss:  tensor(1.3932)\n",
      "Loss: 14.488831520080566\n",
      "h_loss:  tensor(4.7179)  l_loss:  tensor(8.2390)  bl_loss:  tensor(1.3517)\n",
      "Loss: 14.308591842651367\n",
      "h_loss:  tensor(4.8277)  l_loss:  tensor(9.1821)  bl_loss:  tensor(1.3691)\n",
      "Loss: 15.37889289855957\n",
      "h_loss:  tensor(4.5690)  l_loss:  tensor(7.1618)  bl_loss:  tensor(1.1526)\n",
      "Loss: 12.883353233337402\n",
      "Epoch: 23\n",
      "h_loss:  tensor(4.4605)  l_loss:  tensor(6.5170)  bl_loss:  tensor(1.1494)\n",
      "Loss: 12.126923561096191\n",
      "h_loss:  tensor(4.6304)  l_loss:  tensor(9.7782)  bl_loss:  tensor(1.1882)\n",
      "Loss: 15.596758842468262\n",
      "###### Stage 1 Start (Second Stage) ######\n",
      "h_loss:  tensor(4.9299)  l_loss:  tensor(227.5225)  bl_loss:  tensor(23.1770)\n",
      "Loss: 255.62948608398438\n",
      "h_loss:  tensor(4.3834)  l_loss:  tensor(155.2940)  bl_loss:  tensor(24.1952)\n",
      "Loss: 183.87261962890625\n",
      "Epoch: 24\n",
      "h_loss:  tensor(3.8635)  l_loss:  tensor(149.5908)  bl_loss:  tensor(25.0195)\n",
      "Loss: 178.4737091064453\n",
      "h_loss:  tensor(4.7111)  l_loss:  tensor(181.6006)  bl_loss:  tensor(24.8956)\n",
      "Loss: 211.207275390625\n",
      "h_loss:  tensor(4.1106)  l_loss:  tensor(150.9473)  bl_loss:  tensor(23.2295)\n",
      "Loss: 178.28736877441406\n",
      "h_loss:  tensor(4.9609)  l_loss:  tensor(190.9439)  bl_loss:  tensor(23.5306)\n",
      "Loss: 219.43545532226562\n",
      "Epoch: 25\n",
      "h_loss:  tensor(3.8349)  l_loss:  tensor(150.2168)  bl_loss:  tensor(24.0991)\n",
      "Loss: 178.15087890625\n",
      "h_loss:  tensor(4.0276)  l_loss:  tensor(155.7914)  bl_loss:  tensor(24.7358)\n",
      "Loss: 184.5547332763672\n",
      "h_loss:  tensor(4.4524)  l_loss:  tensor(139.6000)  bl_loss:  tensor(21.9918)\n",
      "Loss: 166.0442657470703\n",
      "h_loss:  tensor(5.3208)  l_loss:  tensor(221.1147)  bl_loss:  tensor(24.3164)\n",
      "Loss: 250.75189208984375\n",
      "Epoch: 26\n",
      "h_loss:  tensor(4.0402)  l_loss:  tensor(172.9485)  bl_loss:  tensor(23.8138)\n",
      "Loss: 200.8024444580078\n",
      "h_loss:  tensor(4.6141)  l_loss:  tensor(138.6768)  bl_loss:  tensor(22.8494)\n",
      "Loss: 166.1402587890625\n",
      "h_loss:  tensor(4.3804)  l_loss:  tensor(163.4023)  bl_loss:  tensor(25.3752)\n",
      "Loss: 193.1578826904297\n",
      "h_loss:  tensor(4.5758)  l_loss:  tensor(192.8006)  bl_loss:  tensor(23.0747)\n",
      "Loss: 220.45108032226562\n",
      "Epoch: 27\n",
      "h_loss:  tensor(4.2310)  l_loss:  tensor(163.7505)  bl_loss:  tensor(22.5761)\n",
      "Loss: 190.55763244628906\n",
      "h_loss:  tensor(4.6712)  l_loss:  tensor(177.5694)  bl_loss:  tensor(25.0213)\n",
      "Loss: 207.26185607910156\n",
      "h_loss:  tensor(4.6232)  l_loss:  tensor(163.8087)  bl_loss:  tensor(25.1113)\n",
      "Loss: 193.54315185546875\n",
      "h_loss:  tensor(4.1692)  l_loss:  tensor(159.2639)  bl_loss:  tensor(24.7859)\n",
      "Loss: 188.21905517578125\n",
      "Epoch: 28\n",
      "h_loss:  tensor(4.7843)  l_loss:  tensor(170.1863)  bl_loss:  tensor(23.8443)\n",
      "Loss: 198.8148193359375\n",
      "h_loss:  tensor(4.5433)  l_loss:  tensor(190.7356)  bl_loss:  tensor(25.4629)\n",
      "Loss: 220.7418212890625\n",
      "h_loss:  tensor(3.7767)  l_loss:  tensor(133.3035)  bl_loss:  tensor(23.7963)\n",
      "Loss: 160.8765869140625\n",
      "h_loss:  tensor(4.4612)  l_loss:  tensor(168.6626)  bl_loss:  tensor(23.2685)\n",
      "Loss: 196.39234924316406\n",
      "Epoch: 29\n",
      "h_loss:  tensor(4.6040)  l_loss:  tensor(185.1871)  bl_loss:  tensor(24.1651)\n",
      "Loss: 213.9561767578125\n",
      "h_loss:  tensor(4.3141)  l_loss:  tensor(173.7414)  bl_loss:  tensor(24.1494)\n",
      "Loss: 202.20489501953125\n",
      "h_loss:  tensor(3.8332)  l_loss:  tensor(111.2802)  bl_loss:  tensor(21.0878)\n",
      "Loss: 136.20114135742188\n",
      "h_loss:  tensor(4.8901)  l_loss:  tensor(202.1375)  bl_loss:  tensor(23.3204)\n",
      "Loss: 230.34803771972656\n",
      "Epoch: 30\n",
      "h_loss:  tensor(4.2419)  l_loss:  tensor(176.1671)  bl_loss:  tensor(23.8172)\n",
      "Loss: 204.22616577148438\n",
      "h_loss:  tensor(4.0755)  l_loss:  tensor(138.6528)  bl_loss:  tensor(22.5565)\n",
      "Loss: 165.28482055664062\n",
      "h_loss:  tensor(4.7715)  l_loss:  tensor(160.0385)  bl_loss:  tensor(25.0199)\n",
      "Loss: 189.82981872558594\n",
      "h_loss:  tensor(4.5258)  l_loss:  tensor(185.3088)  bl_loss:  tensor(23.6839)\n",
      "Loss: 213.51856994628906\n",
      "Epoch: 31\n",
      "h_loss:  tensor(4.4337)  l_loss:  tensor(198.9787)  bl_loss:  tensor(23.1676)\n",
      "Loss: 226.58001708984375\n",
      "h_loss:  tensor(4.3375)  l_loss:  tensor(148.8008)  bl_loss:  tensor(24.7025)\n",
      "Loss: 177.8407745361328\n",
      "h_loss:  tensor(4.0779)  l_loss:  tensor(140.8849)  bl_loss:  tensor(22.6080)\n",
      "Loss: 167.57081604003906\n",
      "h_loss:  tensor(4.8981)  l_loss:  tensor(173.8479)  bl_loss:  tensor(25.0933)\n",
      "Loss: 203.83926391601562\n",
      "Epoch: 32\n",
      "h_loss:  tensor(5.1883)  l_loss:  tensor(188.3342)  bl_loss:  tensor(26.0440)\n",
      "Loss: 219.56658935546875\n",
      "h_loss:  tensor(3.9980)  l_loss:  tensor(149.4522)  bl_loss:  tensor(23.7470)\n",
      "Loss: 177.19725036621094\n",
      "h_loss:  tensor(4.3796)  l_loss:  tensor(143.1812)  bl_loss:  tensor(23.0532)\n",
      "Loss: 170.6139678955078\n",
      "h_loss:  tensor(4.0654)  l_loss:  tensor(179.4007)  bl_loss:  tensor(25.1677)\n",
      "Loss: 208.63375854492188\n",
      "Epoch: 33\n",
      "h_loss:  tensor(4.0585)  l_loss:  tensor(135.9150)  bl_loss:  tensor(23.3156)\n",
      "Loss: 163.2891387939453\n",
      "h_loss:  tensor(5.2214)  l_loss:  tensor(195.7390)  bl_loss:  tensor(25.6233)\n",
      "Loss: 226.58364868164062\n",
      "h_loss:  tensor(4.2038)  l_loss:  tensor(172.0158)  bl_loss:  tensor(22.9152)\n",
      "Loss: 199.13478088378906\n",
      "h_loss:  tensor(4.2259)  l_loss:  tensor(156.7880)  bl_loss:  tensor(23.5614)\n",
      "Loss: 184.57534790039062\n",
      "Epoch: 34\n",
      "h_loss:  tensor(4.6081)  l_loss:  tensor(153.2671)  bl_loss:  tensor(23.2872)\n",
      "Loss: 181.1624298095703\n",
      "h_loss:  tensor(4.3418)  l_loss:  tensor(159.5208)  bl_loss:  tensor(23.4837)\n",
      "Loss: 187.34632873535156\n",
      "h_loss:  tensor(4.7770)  l_loss:  tensor(188.7381)  bl_loss:  tensor(22.9896)\n",
      "Loss: 216.5047607421875\n",
      "h_loss:  tensor(3.8862)  l_loss:  tensor(158.9273)  bl_loss:  tensor(24.0319)\n",
      "Loss: 186.8453826904297\n",
      "Epoch: 35\n",
      "h_loss:  tensor(4.5961)  l_loss:  tensor(164.8253)  bl_loss:  tensor(24.4102)\n",
      "Loss: 193.8315887451172\n",
      "h_loss:  tensor(4.0629)  l_loss:  tensor(171.3158)  bl_loss:  tensor(24.6431)\n",
      "Loss: 200.02188110351562\n",
      "h_loss:  tensor(4.4388)  l_loss:  tensor(170.9691)  bl_loss:  tensor(22.5503)\n",
      "Loss: 197.9581756591797\n",
      "h_loss:  tensor(4.5614)  l_loss:  tensor(154.8390)  bl_loss:  tensor(23.1298)\n",
      "Loss: 182.53024291992188\n",
      "Epoch: 36\n",
      "h_loss:  tensor(4.1807)  l_loss:  tensor(144.8664)  bl_loss:  tensor(22.9596)\n",
      "Loss: 172.0066375732422\n",
      "h_loss:  tensor(4.6373)  l_loss:  tensor(162.4466)  bl_loss:  tensor(24.4127)\n",
      "Loss: 191.49661254882812\n",
      "h_loss:  tensor(4.2827)  l_loss:  tensor(174.4544)  bl_loss:  tensor(23.1262)\n",
      "Loss: 201.86329650878906\n",
      "h_loss:  tensor(4.5354)  l_loss:  tensor(183.4365)  bl_loss:  tensor(23.4846)\n",
      "Loss: 211.45648193359375\n",
      "Epoch: 37\n",
      "h_loss:  tensor(4.5778)  l_loss:  tensor(193.3018)  bl_loss:  tensor(25.9002)\n",
      "Loss: 223.7798614501953\n",
      "h_loss:  tensor(4.2100)  l_loss:  tensor(172.0480)  bl_loss:  tensor(24.4049)\n",
      "Loss: 200.66293334960938\n",
      "h_loss:  tensor(4.8452)  l_loss:  tensor(154.2736)  bl_loss:  tensor(22.5623)\n",
      "Loss: 181.6812286376953\n",
      "h_loss:  tensor(3.9506)  l_loss:  tensor(142.3735)  bl_loss:  tensor(22.3386)\n",
      "Loss: 168.66278076171875\n",
      "Epoch: 38\n",
      "h_loss:  tensor(4.2621)  l_loss:  tensor(139.5803)  bl_loss:  tensor(23.3410)\n",
      "Loss: 167.18336486816406\n",
      "h_loss:  tensor(4.7079)  l_loss:  tensor(214.1078)  bl_loss:  tensor(23.4754)\n",
      "Loss: 242.29115295410156\n",
      "h_loss:  tensor(3.9781)  l_loss:  tensor(146.9861)  bl_loss:  tensor(24.0058)\n",
      "Loss: 174.97000122070312\n",
      "h_loss:  tensor(4.6147)  l_loss:  tensor(165.2862)  bl_loss:  tensor(24.3995)\n",
      "Loss: 194.30044555664062\n",
      "Epoch: 39\n",
      "h_loss:  tensor(3.8388)  l_loss:  tensor(117.8935)  bl_loss:  tensor(22.0732)\n",
      "Loss: 143.8054656982422\n",
      "h_loss:  tensor(4.5563)  l_loss:  tensor(186.0422)  bl_loss:  tensor(23.8893)\n",
      "Loss: 214.4877471923828\n",
      "h_loss:  tensor(4.3838)  l_loss:  tensor(190.2557)  bl_loss:  tensor(24.6785)\n",
      "Loss: 219.3180694580078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_loss:  tensor(4.8004)  l_loss:  tensor(169.1630)  bl_loss:  tensor(24.1967)\n",
      "Loss: 198.16009521484375\n",
      "Epoch: 40\n",
      "h_loss:  tensor(4.1290)  l_loss:  tensor(157.0948)  bl_loss:  tensor(22.1938)\n",
      "Loss: 183.41763305664062\n",
      "h_loss:  tensor(4.5296)  l_loss:  tensor(165.9833)  bl_loss:  tensor(24.7242)\n",
      "Loss: 195.23716735839844\n",
      "h_loss:  tensor(4.5800)  l_loss:  tensor(164.4141)  bl_loss:  tensor(24.5119)\n",
      "Loss: 193.50599670410156\n",
      "h_loss:  tensor(4.4020)  l_loss:  tensor(170.2852)  bl_loss:  tensor(24.0540)\n",
      "Loss: 198.7412567138672\n",
      "Epoch: 41\n",
      "h_loss:  tensor(5.1721)  l_loss:  tensor(186.9669)  bl_loss:  tensor(25.4044)\n",
      "Loss: 217.54339599609375\n",
      "h_loss:  tensor(4.0933)  l_loss:  tensor(139.8001)  bl_loss:  tensor(22.2795)\n",
      "Loss: 166.17279052734375\n",
      "h_loss:  tensor(4.1816)  l_loss:  tensor(170.5488)  bl_loss:  tensor(24.8254)\n",
      "Loss: 199.55584716796875\n",
      "h_loss:  tensor(4.1898)  l_loss:  tensor(165.3343)  bl_loss:  tensor(22.3409)\n",
      "Loss: 191.8651123046875\n",
      "Epoch: 42\n",
      "h_loss:  tensor(4.3355)  l_loss:  tensor(159.7710)  bl_loss:  tensor(23.9761)\n",
      "Loss: 188.08261108398438\n",
      "h_loss:  tensor(4.7893)  l_loss:  tensor(183.1753)  bl_loss:  tensor(23.5070)\n",
      "Loss: 211.47169494628906\n",
      "h_loss:  tensor(3.9978)  l_loss:  tensor(167.4347)  bl_loss:  tensor(23.2750)\n",
      "Loss: 194.70748901367188\n",
      "h_loss:  tensor(4.4832)  l_loss:  tensor(149.8904)  bl_loss:  tensor(22.9418)\n",
      "Loss: 177.3153839111328\n",
      "Epoch: 43\n",
      "h_loss:  tensor(4.5796)  l_loss:  tensor(183.0822)  bl_loss:  tensor(24.6375)\n",
      "Loss: 212.2992706298828\n",
      "h_loss:  tensor(4.5463)  l_loss:  tensor(187.3686)  bl_loss:  tensor(23.9372)\n",
      "Loss: 215.85215759277344\n",
      "h_loss:  tensor(4.2302)  l_loss:  tensor(114.6451)  bl_loss:  tensor(21.0606)\n",
      "Loss: 139.935791015625\n",
      "h_loss:  tensor(4.1799)  l_loss:  tensor(178.9955)  bl_loss:  tensor(23.1948)\n",
      "Loss: 206.3701934814453\n",
      "Epoch: 44\n",
      "h_loss:  tensor(3.8363)  l_loss:  tensor(135.7305)  bl_loss:  tensor(23.4960)\n",
      "Loss: 163.0628662109375\n",
      "h_loss:  tensor(4.0957)  l_loss:  tensor(149.9158)  bl_loss:  tensor(22.9416)\n",
      "Loss: 176.9530792236328\n",
      "h_loss:  tensor(4.9974)  l_loss:  tensor(181.9152)  bl_loss:  tensor(23.5934)\n",
      "Loss: 210.50599670410156\n",
      "h_loss:  tensor(4.7101)  l_loss:  tensor(186.4591)  bl_loss:  tensor(24.0103)\n",
      "Loss: 215.1795196533203\n",
      "Epoch: 45\n",
      "h_loss:  tensor(4.0654)  l_loss:  tensor(180.0195)  bl_loss:  tensor(24.8219)\n",
      "Loss: 208.90682983398438\n",
      "h_loss:  tensor(4.5209)  l_loss:  tensor(178.8263)  bl_loss:  tensor(25.0068)\n",
      "Loss: 208.35397338867188\n",
      "h_loss:  tensor(4.7196)  l_loss:  tensor(183.3655)  bl_loss:  tensor(23.5411)\n",
      "Loss: 211.626220703125\n",
      "h_loss:  tensor(4.2128)  l_loss:  tensor(119.6092)  bl_loss:  tensor(20.5570)\n",
      "Loss: 144.37893676757812\n",
      "Epoch: 46\n",
      "h_loss:  tensor(4.3791)  l_loss:  tensor(171.4834)  bl_loss:  tensor(23.9999)\n",
      "Loss: 199.8623504638672\n",
      "h_loss:  tensor(3.8650)  l_loss:  tensor(124.3978)  bl_loss:  tensor(21.0130)\n",
      "Loss: 149.27584838867188\n",
      "h_loss:  tensor(5.2504)  l_loss:  tensor(189.0501)  bl_loss:  tensor(25.0797)\n",
      "Loss: 219.38021850585938\n",
      "h_loss:  tensor(4.2100)  l_loss:  tensor(172.2657)  bl_loss:  tensor(24.2044)\n",
      "Loss: 200.68011474609375\n",
      "Epoch: 47\n",
      "h_loss:  tensor(4.1469)  l_loss:  tensor(160.4769)  bl_loss:  tensor(24.2392)\n",
      "Loss: 188.86300659179688\n",
      "h_loss:  tensor(4.7454)  l_loss:  tensor(188.5756)  bl_loss:  tensor(25.6516)\n",
      "Loss: 218.9725799560547\n",
      "h_loss:  tensor(4.4829)  l_loss:  tensor(136.2039)  bl_loss:  tensor(21.4079)\n",
      "Loss: 162.0946807861328\n",
      "h_loss:  tensor(4.2124)  l_loss:  tensor(173.8688)  bl_loss:  tensor(22.9714)\n",
      "Loss: 201.0525360107422\n",
      "Epoch: 48\n",
      "h_loss:  tensor(4.0020)  l_loss:  tensor(142.5508)  bl_loss:  tensor(23.5770)\n",
      "Loss: 170.12977600097656\n",
      "h_loss:  tensor(4.9791)  l_loss:  tensor(218.5086)  bl_loss:  tensor(24.8370)\n",
      "Loss: 248.3247528076172\n",
      "h_loss:  tensor(3.7874)  l_loss:  tensor(146.2299)  bl_loss:  tensor(22.0597)\n",
      "Loss: 172.07696533203125\n",
      "h_loss:  tensor(4.8452)  l_loss:  tensor(153.8286)  bl_loss:  tensor(22.3406)\n",
      "Loss: 181.01437377929688\n",
      "Epoch: 49\n",
      "h_loss:  tensor(3.4842)  l_loss:  tensor(128.4783)  bl_loss:  tensor(20.9940)\n",
      "Loss: 152.95657348632812\n",
      "h_loss:  tensor(4.7723)  l_loss:  tensor(186.3773)  bl_loss:  tensor(24.6632)\n",
      "Loss: 215.81283569335938\n",
      "h_loss:  tensor(4.6500)  l_loss:  tensor(139.8277)  bl_loss:  tensor(21.6110)\n",
      "Loss: 166.088623046875\n",
      "h_loss:  tensor(4.6330)  l_loss:  tensor(214.7894)  bl_loss:  tensor(22.5131)\n",
      "Loss: 241.9355010986328\n",
      "Epoch: 50\n",
      "h_loss:  tensor(5.1039)  l_loss:  tensor(209.1938)  bl_loss:  tensor(23.8261)\n",
      "Loss: 238.12371826171875\n",
      "h_loss:  tensor(4.0137)  l_loss:  tensor(167.6896)  bl_loss:  tensor(23.8500)\n",
      "Loss: 195.55332946777344\n",
      "h_loss:  tensor(3.8427)  l_loss:  tensor(129.1086)  bl_loss:  tensor(21.6555)\n",
      "Loss: 154.60684204101562\n",
      "h_loss:  tensor(4.6116)  l_loss:  tensor(154.3792)  bl_loss:  tensor(22.8489)\n",
      "Loss: 181.8396453857422\n",
      "Epoch: 51\n",
      "h_loss:  tensor(4.1923)  l_loss:  tensor(164.8009)  bl_loss:  tensor(23.5986)\n",
      "Loss: 192.59170532226562\n",
      "h_loss:  tensor(4.5796)  l_loss:  tensor(167.5971)  bl_loss:  tensor(24.0018)\n",
      "Loss: 196.17848205566406\n",
      "h_loss:  tensor(4.4361)  l_loss:  tensor(149.5635)  bl_loss:  tensor(23.1444)\n",
      "Loss: 177.14401245117188\n",
      "h_loss:  tensor(4.3655)  l_loss:  tensor(179.6020)  bl_loss:  tensor(23.5639)\n",
      "Loss: 207.5313262939453\n",
      "Epoch: 52\n",
      "h_loss:  tensor(5.1995)  l_loss:  tensor(194.5031)  bl_loss:  tensor(25.0818)\n",
      "Loss: 224.78439331054688\n",
      "h_loss:  tensor(4.6101)  l_loss:  tensor(187.2279)  bl_loss:  tensor(23.7701)\n",
      "Loss: 215.60812377929688\n",
      "h_loss:  tensor(3.9041)  l_loss:  tensor(150.5962)  bl_loss:  tensor(23.0456)\n",
      "Loss: 177.5458984375\n",
      "h_loss:  tensor(3.9565)  l_loss:  tensor(130.1902)  bl_loss:  tensor(22.2121)\n",
      "Loss: 156.3588409423828\n",
      "Epoch: 53\n",
      "h_loss:  tensor(3.8956)  l_loss:  tensor(134.1639)  bl_loss:  tensor(21.6063)\n",
      "Loss: 159.6657257080078\n",
      "h_loss:  tensor(4.5279)  l_loss:  tensor(189.5051)  bl_loss:  tensor(24.2417)\n",
      "Loss: 218.2747039794922\n",
      "h_loss:  tensor(4.9306)  l_loss:  tensor(195.4991)  bl_loss:  tensor(24.5814)\n",
      "Loss: 225.0110321044922\n",
      "h_loss:  tensor(4.2116)  l_loss:  tensor(149.1815)  bl_loss:  tensor(22.4150)\n",
      "Loss: 175.8080596923828\n",
      "Epoch: 54\n",
      "h_loss:  tensor(4.3810)  l_loss:  tensor(160.1349)  bl_loss:  tensor(24.5171)\n",
      "Loss: 189.03298950195312\n",
      "h_loss:  tensor(3.8338)  l_loss:  tensor(125.2010)  bl_loss:  tensor(21.0089)\n",
      "Loss: 150.04373168945312\n",
      "h_loss:  tensor(4.9819)  l_loss:  tensor(192.2488)  bl_loss:  tensor(25.1256)\n",
      "Loss: 222.35635375976562\n",
      "h_loss:  tensor(4.3553)  l_loss:  tensor(185.4178)  bl_loss:  tensor(21.6493)\n",
      "Loss: 211.42242431640625\n",
      "Epoch: 55\n",
      "h_loss:  tensor(5.2417)  l_loss:  tensor(197.7435)  bl_loss:  tensor(24.6049)\n",
      "Loss: 227.59014892578125\n",
      "h_loss:  tensor(3.8321)  l_loss:  tensor(142.3082)  bl_loss:  tensor(22.5036)\n",
      "Loss: 168.6438446044922\n",
      "h_loss:  tensor(4.0264)  l_loss:  tensor(141.5228)  bl_loss:  tensor(23.5172)\n",
      "Loss: 169.06639099121094\n",
      "h_loss:  tensor(4.6398)  l_loss:  tensor(172.8521)  bl_loss:  tensor(22.5862)\n",
      "Loss: 200.07815551757812\n",
      "Epoch: 56\n",
      "h_loss:  tensor(4.2265)  l_loss:  tensor(161.5844)  bl_loss:  tensor(23.4522)\n",
      "Loss: 189.2630615234375\n",
      "h_loss:  tensor(4.5653)  l_loss:  tensor(193.9762)  bl_loss:  tensor(24.8989)\n",
      "Loss: 223.44041442871094\n",
      "h_loss:  tensor(3.8348)  l_loss:  tensor(125.7902)  bl_loss:  tensor(20.7268)\n",
      "Loss: 150.351806640625\n",
      "h_loss:  tensor(4.9974)  l_loss:  tensor(181.5401)  bl_loss:  tensor(23.4693)\n",
      "Loss: 210.0068359375\n",
      "Epoch: 57\n",
      "h_loss:  tensor(3.5129)  l_loss:  tensor(125.8811)  bl_loss:  tensor(21.1315)\n",
      "Loss: 150.52548217773438\n",
      "h_loss:  tensor(4.9735)  l_loss:  tensor(180.5766)  bl_loss:  tensor(23.9117)\n",
      "Loss: 209.46177673339844\n",
      "h_loss:  tensor(4.4402)  l_loss:  tensor(175.4403)  bl_loss:  tensor(23.1850)\n",
      "Loss: 203.06546020507812\n",
      "h_loss:  tensor(4.6782)  l_loss:  tensor(176.3852)  bl_loss:  tensor(24.1306)\n",
      "Loss: 205.1940155029297\n",
      "Epoch: 58\n",
      "h_loss:  tensor(4.7816)  l_loss:  tensor(159.9659)  bl_loss:  tensor(23.6858)\n",
      "Loss: 188.43328857421875\n",
      "h_loss:  tensor(3.7246)  l_loss:  tensor(124.8970)  bl_loss:  tensor(20.9748)\n",
      "Loss: 149.59637451171875\n",
      "h_loss:  tensor(4.4106)  l_loss:  tensor(170.8189)  bl_loss:  tensor(23.3019)\n",
      "Loss: 198.53140258789062\n",
      "h_loss:  tensor(4.6607)  l_loss:  tensor(210.0915)  bl_loss:  tensor(24.2685)\n",
      "Loss: 239.02078247070312\n",
      "Epoch: 59\n",
      "h_loss:  tensor(3.8611)  l_loss:  tensor(159.6017)  bl_loss:  tensor(24.2765)\n",
      "Loss: 187.7393341064453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_loss:  tensor(4.7787)  l_loss:  tensor(187.5374)  bl_loss:  tensor(25.7771)\n",
      "Loss: 218.09317016601562\n",
      "h_loss:  tensor(4.8009)  l_loss:  tensor(151.5484)  bl_loss:  tensor(22.8861)\n",
      "Loss: 179.2354736328125\n",
      "h_loss:  tensor(4.1722)  l_loss:  tensor(160.6225)  bl_loss:  tensor(22.4854)\n",
      "Loss: 187.2801971435547\n",
      "Epoch: 60\n",
      "h_loss:  tensor(3.6970)  l_loss:  tensor(155.1088)  bl_loss:  tensor(23.3595)\n",
      "Loss: 182.16529846191406\n",
      "h_loss:  tensor(4.7470)  l_loss:  tensor(157.6633)  bl_loss:  tensor(24.4269)\n",
      "Loss: 186.8371124267578\n",
      "h_loss:  tensor(4.6506)  l_loss:  tensor(174.0318)  bl_loss:  tensor(23.2765)\n",
      "Loss: 201.95887756347656\n",
      "h_loss:  tensor(4.5287)  l_loss:  tensor(175.5602)  bl_loss:  tensor(23.5838)\n",
      "Loss: 203.67274475097656\n",
      "Epoch: 61\n",
      "h_loss:  tensor(3.8273)  l_loss:  tensor(144.7319)  bl_loss:  tensor(21.4426)\n",
      "Loss: 170.0018310546875\n",
      "h_loss:  tensor(4.6504)  l_loss:  tensor(141.2365)  bl_loss:  tensor(22.2457)\n",
      "Loss: 168.13253784179688\n",
      "h_loss:  tensor(4.6965)  l_loss:  tensor(212.1248)  bl_loss:  tensor(24.3789)\n",
      "Loss: 241.2001953125\n",
      "h_loss:  tensor(4.3588)  l_loss:  tensor(161.7578)  bl_loss:  tensor(25.2324)\n",
      "Loss: 191.34893798828125\n",
      "Epoch: 62\n",
      "h_loss:  tensor(3.8273)  l_loss:  tensor(144.4298)  bl_loss:  tensor(21.4392)\n",
      "Loss: 169.6962432861328\n",
      "h_loss:  tensor(4.3460)  l_loss:  tensor(139.4668)  bl_loss:  tensor(22.7869)\n",
      "Loss: 166.5996856689453\n",
      "h_loss:  tensor(4.5973)  l_loss:  tensor(190.5187)  bl_loss:  tensor(24.6463)\n",
      "Loss: 219.7622833251953\n",
      "h_loss:  tensor(4.8049)  l_loss:  tensor(183.0102)  bl_loss:  tensor(24.4289)\n",
      "Loss: 212.2439422607422\n",
      "Epoch: 63\n",
      "h_loss:  tensor(4.5135)  l_loss:  tensor(162.5167)  bl_loss:  tensor(23.5738)\n",
      "Loss: 190.60391235351562\n",
      "h_loss:  tensor(4.2442)  l_loss:  tensor(176.8846)  bl_loss:  tensor(23.5984)\n",
      "Loss: 204.72711181640625\n",
      "h_loss:  tensor(4.3072)  l_loss:  tensor(162.2170)  bl_loss:  tensor(23.0689)\n",
      "Loss: 189.5930938720703\n",
      "h_loss:  tensor(4.5821)  l_loss:  tensor(157.5427)  bl_loss:  tensor(22.6470)\n",
      "Loss: 184.7718505859375\n",
      "Epoch: 64\n",
      "h_loss:  tensor(4.4014)  l_loss:  tensor(151.9691)  bl_loss:  tensor(23.2897)\n",
      "Loss: 179.66018676757812\n",
      "h_loss:  tensor(4.3072)  l_loss:  tensor(162.2047)  bl_loss:  tensor(23.0766)\n",
      "Loss: 189.5885772705078\n",
      "h_loss:  tensor(4.8273)  l_loss:  tensor(185.1946)  bl_loss:  tensor(23.2584)\n",
      "Loss: 213.28028869628906\n",
      "h_loss:  tensor(4.0893)  l_loss:  tensor(161.0733)  bl_loss:  tensor(23.1758)\n",
      "Loss: 188.3384246826172\n",
      "Epoch: 65\n",
      "h_loss:  tensor(4.1828)  l_loss:  tensor(171.5930)  bl_loss:  tensor(24.0012)\n",
      "Loss: 199.77700805664062\n",
      "h_loss:  tensor(4.2363)  l_loss:  tensor(144.8992)  bl_loss:  tensor(22.4308)\n",
      "Loss: 171.56637573242188\n",
      "h_loss:  tensor(4.2770)  l_loss:  tensor(166.1767)  bl_loss:  tensor(23.5092)\n",
      "Loss: 193.96290588378906\n",
      "h_loss:  tensor(4.9640)  l_loss:  tensor(180.6512)  bl_loss:  tensor(23.7799)\n",
      "Loss: 209.39512634277344\n",
      "Epoch: 66\n",
      "h_loss:  tensor(4.0449)  l_loss:  tensor(163.7396)  bl_loss:  tensor(23.2326)\n",
      "Loss: 191.01710510253906\n",
      "h_loss:  tensor(4.7279)  l_loss:  tensor(218.3448)  bl_loss:  tensor(23.9805)\n",
      "Loss: 247.05323791503906\n",
      "h_loss:  tensor(4.5892)  l_loss:  tensor(146.7634)  bl_loss:  tensor(23.2024)\n",
      "Loss: 174.55499267578125\n",
      "h_loss:  tensor(4.1917)  l_loss:  tensor(134.6681)  bl_loss:  tensor(21.5032)\n",
      "Loss: 160.36306762695312\n",
      "Epoch: 67\n",
      "h_loss:  tensor(4.9928)  l_loss:  tensor(187.4566)  bl_loss:  tensor(23.5672)\n",
      "Loss: 216.0166473388672\n",
      "h_loss:  tensor(4.1522)  l_loss:  tensor(164.9530)  bl_loss:  tensor(23.0459)\n",
      "Loss: 192.15106201171875\n",
      "h_loss:  tensor(4.3130)  l_loss:  tensor(161.4635)  bl_loss:  tensor(23.4414)\n",
      "Loss: 189.21788024902344\n",
      "h_loss:  tensor(4.1460)  l_loss:  tensor(148.6017)  bl_loss:  tensor(22.0539)\n",
      "Loss: 174.801513671875\n",
      "Epoch: 68\n",
      "h_loss:  tensor(3.9232)  l_loss:  tensor(128.3515)  bl_loss:  tensor(21.2866)\n",
      "Loss: 153.56124877929688\n",
      "h_loss:  tensor(4.7501)  l_loss:  tensor(191.3292)  bl_loss:  tensor(24.1410)\n",
      "Loss: 220.22030639648438\n",
      "h_loss:  tensor(4.3143)  l_loss:  tensor(157.2003)  bl_loss:  tensor(23.3988)\n",
      "Loss: 184.91339111328125\n",
      "h_loss:  tensor(4.6052)  l_loss:  tensor(196.3867)  bl_loss:  tensor(22.0734)\n",
      "Loss: 223.06527709960938\n",
      "Epoch: 69\n",
      "h_loss:  tensor(4.5283)  l_loss:  tensor(155.5253)  bl_loss:  tensor(23.4431)\n",
      "Loss: 183.49673461914062\n",
      "h_loss:  tensor(4.0933)  l_loss:  tensor(138.9990)  bl_loss:  tensor(21.7692)\n",
      "Loss: 164.8614044189453\n",
      "h_loss:  tensor(4.0276)  l_loss:  tensor(157.0565)  bl_loss:  tensor(23.6345)\n",
      "Loss: 184.718505859375\n",
      "h_loss:  tensor(4.9250)  l_loss:  tensor(214.4243)  bl_loss:  tensor(23.8755)\n",
      "Loss: 243.22482299804688\n",
      "Epoch: 70\n",
      "h_loss:  tensor(4.0000)  l_loss:  tensor(124.4829)  bl_loss:  tensor(21.1617)\n",
      "Loss: 149.64451599121094\n",
      "h_loss:  tensor(4.7382)  l_loss:  tensor(210.5585)  bl_loss:  tensor(23.9675)\n",
      "Loss: 239.2642059326172\n",
      "h_loss:  tensor(4.3581)  l_loss:  tensor(164.5662)  bl_loss:  tensor(24.7250)\n",
      "Loss: 193.6493682861328\n",
      "h_loss:  tensor(4.4505)  l_loss:  tensor(158.7102)  bl_loss:  tensor(22.1048)\n",
      "Loss: 185.2654571533203\n",
      "Epoch: 71\n",
      "h_loss:  tensor(4.2378)  l_loss:  tensor(178.2299)  bl_loss:  tensor(23.4000)\n",
      "Loss: 205.86782836914062\n",
      "h_loss:  tensor(4.0136)  l_loss:  tensor(136.8083)  bl_loss:  tensor(22.2148)\n",
      "Loss: 163.0366973876953\n",
      "h_loss:  tensor(4.9785)  l_loss:  tensor(205.0337)  bl_loss:  tensor(25.1909)\n",
      "Loss: 235.2030792236328\n",
      "h_loss:  tensor(4.3447)  l_loss:  tensor(141.8135)  bl_loss:  tensor(22.4957)\n",
      "Loss: 168.6539764404297\n",
      "Epoch: 72\n",
      "h_loss:  tensor(4.5442)  l_loss:  tensor(181.3993)  bl_loss:  tensor(22.8533)\n",
      "Loss: 208.79672241210938\n",
      "h_loss:  tensor(3.8950)  l_loss:  tensor(107.3609)  bl_loss:  tensor(19.5927)\n",
      "Loss: 130.84864807128906\n",
      "h_loss:  tensor(4.2715)  l_loss:  tensor(178.0840)  bl_loss:  tensor(22.1949)\n",
      "Loss: 204.55035400390625\n",
      "h_loss:  tensor(5.0119)  l_loss:  tensor(199.5996)  bl_loss:  tensor(25.8317)\n",
      "Loss: 230.4432373046875\n",
      "Epoch: 73\n",
      "h_loss:  tensor(4.7559)  l_loss:  tensor(181.0257)  bl_loss:  tensor(23.3421)\n",
      "Loss: 209.12368774414062\n",
      "h_loss:  tensor(3.8080)  l_loss:  tensor(153.9270)  bl_loss:  tensor(22.8335)\n",
      "Loss: 180.56846618652344\n",
      "h_loss:  tensor(4.0055)  l_loss:  tensor(150.0600)  bl_loss:  tensor(22.7167)\n",
      "Loss: 176.78221130371094\n",
      "h_loss:  tensor(5.0037)  l_loss:  tensor(179.6492)  bl_loss:  tensor(23.8944)\n",
      "Loss: 208.54727172851562\n",
      "Epoch: 74\n",
      "h_loss:  tensor(3.8412)  l_loss:  tensor(151.2720)  bl_loss:  tensor(23.0288)\n",
      "Loss: 178.14198303222656\n",
      "h_loss:  tensor(4.5414)  l_loss:  tensor(168.6003)  bl_loss:  tensor(24.4024)\n",
      "Loss: 197.5441131591797\n",
      "h_loss:  tensor(4.1041)  l_loss:  tensor(122.5975)  bl_loss:  tensor(20.4901)\n",
      "Loss: 147.19171142578125\n",
      "h_loss:  tensor(5.1222)  l_loss:  tensor(220.1142)  bl_loss:  tensor(23.4796)\n",
      "Loss: 248.71591186523438\n",
      "Epoch: 75\n",
      "h_loss:  tensor(5.3849)  l_loss:  tensor(217.6777)  bl_loss:  tensor(24.7396)\n",
      "Loss: 247.80218505859375\n",
      "h_loss:  tensor(3.8687)  l_loss:  tensor(122.2436)  bl_loss:  tensor(21.1250)\n",
      "Loss: 147.23728942871094\n",
      "h_loss:  tensor(3.7423)  l_loss:  tensor(139.1187)  bl_loss:  tensor(22.9509)\n",
      "Loss: 165.81179809570312\n",
      "h_loss:  tensor(4.6475)  l_loss:  tensor(180.6864)  bl_loss:  tensor(23.1158)\n",
      "Loss: 208.44976806640625\n",
      "Epoch: 76\n",
      "h_loss:  tensor(4.3675)  l_loss:  tensor(171.7721)  bl_loss:  tensor(23.8396)\n",
      "Loss: 199.9791717529297\n",
      "h_loss:  tensor(4.1724)  l_loss:  tensor(147.6517)  bl_loss:  tensor(22.8529)\n",
      "Loss: 174.67706298828125\n",
      "h_loss:  tensor(4.8067)  l_loss:  tensor(176.7504)  bl_loss:  tensor(23.8423)\n",
      "Loss: 205.3993682861328\n",
      "h_loss:  tensor(4.1673)  l_loss:  tensor(161.1193)  bl_loss:  tensor(23.5687)\n",
      "Loss: 188.8552703857422\n",
      "Epoch: 77\n",
      "h_loss:  tensor(4.9660)  l_loss:  tensor(187.8168)  bl_loss:  tensor(23.7881)\n",
      "Loss: 216.57086181640625\n",
      "h_loss:  tensor(3.8320)  l_loss:  tensor(151.8891)  bl_loss:  tensor(23.5675)\n",
      "Loss: 179.28858947753906\n",
      "h_loss:  tensor(4.1583)  l_loss:  tensor(162.9636)  bl_loss:  tensor(23.0555)\n",
      "Loss: 190.17745971679688\n",
      "h_loss:  tensor(4.6081)  l_loss:  tensor(152.4876)  bl_loss:  tensor(22.9960)\n",
      "Loss: 180.09173583984375\n",
      "Epoch: 78\n",
      "h_loss:  tensor(4.3598)  l_loss:  tensor(141.8268)  bl_loss:  tensor(22.7726)\n",
      "Loss: 168.95919799804688\n",
      "h_loss:  tensor(3.8360)  l_loss:  tensor(151.9239)  bl_loss:  tensor(23.0022)\n",
      "Loss: 178.76206970214844\n",
      "h_loss:  tensor(4.5996)  l_loss:  tensor(182.5695)  bl_loss:  tensor(24.2353)\n",
      "Loss: 211.40435791015625\n",
      "h_loss:  tensor(4.7501)  l_loss:  tensor(191.4822)  bl_loss:  tensor(23.9945)\n",
      "Loss: 220.2269287109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79\n",
      "h_loss:  tensor(4.2364)  l_loss:  tensor(137.2656)  bl_loss:  tensor(21.8982)\n",
      "Loss: 163.40016174316406\n",
      "h_loss:  tensor(5.0071)  l_loss:  tensor(186.4589)  bl_loss:  tensor(23.9399)\n",
      "Loss: 215.40586853027344\n",
      "h_loss:  tensor(3.9758)  l_loss:  tensor(160.0431)  bl_loss:  tensor(23.2269)\n",
      "Loss: 187.24581909179688\n",
      "h_loss:  tensor(4.3804)  l_loss:  tensor(173.6856)  bl_loss:  tensor(23.3231)\n",
      "Loss: 201.38912963867188\n",
      "======Training Done======\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(80):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    for idx, batch in enumerate(loader_train):\n",
    "        # make all the ground truth tensors needed for loss computation\n",
    "        image = batch['image'].float()\n",
    "        pos2d_list = batch['pos_2d'] # size (N, 21, 2)\n",
    "        pos3d_list = batch['pos_3d'] # size (N, 21, 3)\n",
    "        loc_map, heatmap, one_hot = makeMaps(pos2d_list, pos3d_list)\n",
    "        # y_pred is of size 64 x 224 x 224\n",
    "        y_pred = model(image)\n",
    "        \n",
    "        # h_pred is of size 21 x 224 x 224\n",
    "        h_pred = modelHeatmap(y_pred)\n",
    "        \n",
    "        # l_pred is of size 3 x 224 x 224, the 3 representing x, y, z location maps of all 21 joints\n",
    "        l_pred = modelLocmap(y_pred)\n",
    "        \n",
    "        # register a hook so I can print out its gradient\n",
    "        #l_pred.register_hook(lambda grad: printGradientMaxMin(grad, 'l_pred'))\n",
    "        \n",
    "        # use heatmap loss defined in VNect\n",
    "        loss = computeLoss(heatmap, one_hot, loc_map, h_pred, l_pred)\n",
    "        \n",
    "        # print and store the loss curve\n",
    "        print(\"Loss: {}\".format(loss))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # Clears the gradients of all optimized torch.Tensor s\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    torch.save(model.state_dict(), 'model_param.pt')\n",
    "    torch.save(modelHeatmap.state_dict(), 'modelHeatmap_param.pt')\n",
    "    torch.save(modelLocmap.state_dict(), 'modelLocmap_param.pt')\n",
    "    torch.save(optimizer.state_dict(), 'optimizer_param.pt')\n",
    "    torch.save({'losses': losses, 'computeLoss': computeLoss, 'epoch': epoch + 1}, 'training_param.pt')\n",
    "        \n",
    "print(\"======Training Done======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67792f612539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(blb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_param.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelHeatmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'modelHeatmap_param.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelLocmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'modelLocmap_param.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer_param.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#print(blb)\n",
    "torch.save(model.state_dict(), 'model_param.pt')\n",
    "torch.save(modelHeatmap.state_dict(), 'modelHeatmap_param.pt')\n",
    "torch.save(modelLocmap.state_dict(), 'modelLocmap_param.pt')\n",
    "torch.save(optimizer.state_dict(), 'optimizer_param.pt')\n",
    "torch.save({'losses': losses, 'computeLoss': computeLoss, 'epoch': epoch + 1}, 'training_param.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size: [6.0, 4.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23d002678d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"loss\")\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "print(\"Current size:\", fig_size)\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 9\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation...\n"
     ]
    }
   ],
   "source": [
    "def makePosList(h_pred, l_pred):\n",
    "    p2d_y, p2d_x = np.unravel_index(torch.argmax(h_pred.view(num_joints, -1), dim=1).data.numpy(), (image_size, image_size))\n",
    "    p2d = np.stack((p2d_x, p2d_y), axis=-1)\n",
    "\n",
    "    p3d_x = l_pred[0].data.numpy()\n",
    "    p3d_x = p3d_x[p2d_y, p2d_x]\n",
    "\n",
    "    p3d_y = l_pred[1].data.numpy()\n",
    "    p3d_y = p3d_y[p2d_y, p2d_x]\n",
    "\n",
    "    p3d_z = l_pred[2].data.numpy()\n",
    "    p3d_z = p3d_z[p2d_y, p2d_x]\n",
    "\n",
    "    p3d = np.stack((p3d_x, p3d_y, p3d_z), axis=-1)\n",
    "    \n",
    "    return p2d, p3d\n",
    "#b_idx = torch.from_numpy(np.repeat(np.arange(batch_size), num_joints)).long()\n",
    "mplb_idx = np.repeat(np.arange(batch_size), num_joints)\n",
    "def makePosListBatch(h_pred, l_pred):\n",
    "    idx_2d = torch.argmax(h_pred.view(batch_size, num_joints, -1), dim=2).data.numpy()\n",
    "    \n",
    "    p2d_y, p2d_x = np.unravel_index(idx_2d, (image_size, image_size))\n",
    "    p2d = np.stack((p2d_x, p2d_y), axis=-1)\n",
    "\n",
    "    l_pred = l_pred.view(batch_size, 3, -1)\n",
    "    \n",
    "    p3d_x = l_pred[:, 0].data.numpy()\n",
    "    p3d_x = p3d_x[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d_y = l_pred[:, 1].data.numpy()\n",
    "    p3d_y = p3d_y[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d_z = l_pred[:, 2].data.numpy()\n",
    "    p3d_z = p3d_z[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d = np.stack((p3d_x.reshape(batch_size, num_joints), p3d_y.reshape(batch_size, num_joints), p3d_z.reshape(batch_size, num_joints)), axis=-1)\n",
    "    \n",
    "    return p2d, p3d\n",
    "\n",
    "print(\"Evaluation...\")\n",
    "eval_loss = 0\n",
    "eval_iter = 0\n",
    "def eval_net():\n",
    "    for idx, batch in enumerate(loader_train):\n",
    "        eval_iter = idx\n",
    "        image = batch['image'].float()\n",
    "        pos2d_list = batch['pos_2d'] # size (N, 21, 2)\n",
    "        pos3d_list = batch['pos_3d'] # size (N, 21, 3)\n",
    "        loc_map, heatmap, one_hot = makeMaps(pos2d_list, pos3d_list)\n",
    "        y_pred = model(image)\n",
    "        h_pred = modelHeatmap(y_pred)\n",
    "        l_pred = modelLocmap(y_pred)\n",
    "        #print(\"output shape: {}\".format(y_pred.shape))\n",
    "        # use heatmap loss defined in VNect\n",
    "        #loss = computeLoss(heatmap, one_hot, loc_map, h_pred, l_pred)\n",
    "\n",
    "        #eval_loss = eval_loss + loss\n",
    "\n",
    "        if idx == 0:\n",
    "            # show some images\n",
    "            p2d, p3d = makePosList(h_pred[0], l_pred[0])\n",
    "            show_joints(image[0].data.numpy().transpose((1,2,0)), p2d, p3d)\n",
    "            show_joints(image[0].data.numpy().transpose((1,2,0)), batch['pos_2d'][0], batch['pos_3d'][0])\n",
    "            break\n",
    "#eval_net()\n",
    "#eval_iter = eval_iter + 1\n",
    "#print(\"Eval Loss: {}\".format(eval_loss / eval_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fully Connected Layer to Predict Gesture**\n",
    "\n",
    "Input: The outputs of our joint prediction model outputs:\n",
    "\n",
    "    p2d: # size (N, 21, 2)\n",
    "    p3d: # size (N, 21, 3)\n",
    "    \n",
    "Output: \n",
    "\n",
    "    y: # size (N, C=10), where C is the number of gesture classes\n",
    "\n",
    "**Note**: We will use both the 2D positions and the 3D positions of the joints to figure out what the gesture is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the gesture data\n",
    "g_dataset = GestureDataset('gesture_dataset.csv', transform=transform, train=True)\n",
    "\n",
    "N = len(g_dataset)\n",
    "\n",
    "loader_g_train = DataLoader(g_dataset, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8))))\n",
    "\n",
    "loader_g_val = DataLoader(g_dataset, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8), int(N*0.9))))\n",
    "\n",
    "loader_g_test = DataLoader(g_dataset, batch_size=batch_size,\n",
    "                         sampler=sampler.SubsetRandomSampler(range(int(N*0.9),N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our fc should just output probabilities\n",
    "fc = nn.Sequential(\n",
    "    nn.Linear(in_features=105, out_features=105),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=105, out_features=50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=50, out_features=10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# predicted gesture probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the FC loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines constants\n",
    "batch_idx = torch.from_numpy(np.arange(batch_size)).long()\n",
    "\n",
    "def compute_g_loss(p2d, p3d, g_GT):\n",
    "    p2d = torch.from_numpy(p2d)\n",
    "    p3d = torch.from_numpy(p3d)\n",
    "\n",
    "    # put 2D and 3D joint positions together\n",
    "    fc_in = torch.cat((p2d.float(), p3d), dim=2) # shape (N, 21, 5)\n",
    "    fc_in = fc_in.view(batch_size, -1) # shape(N, 105)\n",
    "\n",
    "    # g_GT is of size (N, ) it just contains the labels for the batch\n",
    "    g_pred = fc(fc_in) # shape (N, 10) \n",
    "    g_loss = torch.sum(-1.0 * (g_pred[batch_idx, g_GT.long()] + epsilon).log()) / batch_size\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freeze Joint Prediction Network and Define Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# completely free out JP Net\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in modelHeatmap.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in modelLocmap.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimize fully connected network only\n",
    "g_optimizer = torch.optim.Adam(fc.parameters(), lr=1.0e-4)\n",
    "g_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 8.036097526550293\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 11.19482421875\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 13.355612754821777\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 16.80914878845215\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 12.996598243713379\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 14.209319114685059\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 8.795677185058594\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 16.207300186157227\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 14.637527465820312\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 7.711728572845459\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 14.034470558166504\n",
      "(4, 21, 2) (4, 21, 3)\n",
      "G Loss: 12.32028865814209\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7d2cfed65825>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# y_pred is of size 64 x 224 x 224\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# h_pred is of size 21 x 224 x 224\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    688\u001b[0m         return F.conv_transpose2d(\n\u001b[0;32m    689\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    for idx, batch in enumerate(loader_g_train):\n",
    "        # make all the ground truth tensors needed for loss computation\n",
    "        image = batch['image'].float()\n",
    "        # y_pred is of size 64 x 224 x 224\n",
    "        y_pred = model(image)\n",
    "        \n",
    "        # h_pred is of size 21 x 224 x 224\n",
    "        h_pred = modelHeatmap(y_pred)\n",
    "        \n",
    "        # l_pred is of size 3 x 224 x 224, the 3 representing x, y, z location maps of all 21 joints\n",
    "        l_pred = modelLocmap(y_pred)\n",
    "        \n",
    "        p2d, p3d = makePosListBatch(h_pred, l_pred)\n",
    "        print(p2d.shape, p3d.shape)\n",
    "        # print and store the loss curve\n",
    "        \n",
    "        g_loss = compute_g_loss(p2d, p3d, batch['label'])\n",
    "        \n",
    "        print(\"G Loss: {}\".format(g_loss))\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        \n",
    "        g_optimizer.step()\n",
    "        # Clears the gradients of all optimized torch.Tensor s\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "    torch.save(fc.state_dict(), 'fc_param.pt')\n",
    "    torch.save(g_optimizer.state_dict(), 'g_optimizer_param.pt')\n",
    "    torch.save({'losses': g_losses, 'epoch': epoch + 1}, 'g_training_param.pt')\n",
    "        \n",
    "print(\"======Training Done======\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
