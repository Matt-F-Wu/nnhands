{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resouces:**\n",
    "\n",
    "Explanation on the resnet architecture: input_size/output_size/kernel/stride at each layer:\n",
    "https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624\n",
    "\n",
    "Resnet50 Architecture:\n",
    "https://www.kaggle.com/keras/resnet50\n",
    "\n",
    "Simple way of unpacking resnetX for stripping out FC layers and such:\n",
    "https://discuss.pytorch.org/t/resnet-pretrained-model-with-last-fc-layer-stripped-does-not-work/17951\n",
    "\n",
    "Reason as to why we want to resize each image and their labels to 224 x 224:\n",
    "https://stackoverflow.com/questions/43922308/what-input-image-size-is-correct-for-the-version-of-resnet-v2-in-tensorflow-slim\n",
    "\n",
    "How to modify the FC layer of resnet:\n",
    "https://discuss.pytorch.org/t/how-to-modify-the-final-fc-layer-based-on-the-torch-model/766/3\n",
    "\n",
    "How to partially freeze resnet34:\n",
    "https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing requirement:**\n",
    "\n",
    "In order to define the heatmap loss as torch.nn.functional.cross_entropy(input, target, weight=None, size_average=True, ignore_index=-100, reduce=True):\n",
    "\n",
    "We need to have the target/label take on the form of (N, J, H, W)\n",
    "\n",
    "Each j in J represents a joint\n",
    "\n",
    "**Important:** The image is of size (N, 3, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torchvision import transforms, utils\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from libs.data_utils import HandDataset, ToTensor, Scale, GestureDataset\n",
    "from libs.layer_utils import flatten, random_weight, zero_weight\n",
    "from libs.model_utils import (show_joints, makePosList, makeHeatMapOneHot,\n",
    "                              makeMaps, generate_blw, ComputeLoss, get_loss,\n",
    "                              load_model, save_model)\n",
    "from libs.model import model, modelHeatmap, modelLocmap\n",
    "from libs.lit_data import data2d, data3d, data3d2\n",
    "from libs.misc import write_log\n",
    "                              \n",
    "plt.ion() # interactive mode\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    USE_GPU=False\n",
    "print('Using device:', device)\n",
    "\n",
    "batch_size = 4\n",
    "num_joints = 21\n",
    "image_size = 224\n",
    "dtype = torch.float32\n",
    "g_heatmap_size = 9\n",
    "\n",
    "b_idx = np.repeat(np.arange(batch_size), num_joints)\n",
    "b_idx = torch.from_numpy(b_idx).long()\n",
    "j_idx = np.array(list(np.arange(num_joints))*batch_size)\n",
    "j_idx = torch.from_numpy(j_idx).long()\n",
    "\n",
    "params['batch_size'] = batch_size\n",
    "params['num_joints'] = num_joints\n",
    "params['image_size'] = image_size\n",
    "params['dtype'] = dtype\n",
    "params['device'] = device\n",
    "params['USE_GPU'] = USE_GPU\n",
    "params['g_heatmap_size'] = 9\n",
    "params['b_idx'] = b_idx\n",
    "params['j_idx'] = j_idx\n",
    "\n",
    "blw = generate_blw(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Scale(image_size, image_size),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "hand_train = HandDataset('toy_dataset.csv', transform=transform, train=True, device=device)\n",
    "N = len(hand_train)\n",
    "loader_train = DataLoader(hand_train, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.85))),\n",
    "            drop_last=True)\n",
    "\n",
    "hand_val = HandDataset('toy_dataset.csv', transform=transform, train=True)\n",
    "loader_val = DataLoader(hand_val, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.85), int(N*0.9))),\n",
    "                       drop_last=True)\n",
    "\n",
    "hand_test = HandDataset('toy_dataset.csv', transform=transform, train=False)\n",
    "loader_test = DataLoader(hand_test, batch_size=batch_size,\n",
    "                         sampler=sampler.SubsetRandomSampler(range(int(N*0.9),N)),\n",
    "                        drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "### Define additional params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_every = 600\n",
    "save_every = 1200\n",
    "\n",
    "trainlog_fp = 'trainlog.txt'\n",
    "vallog_fp = 'vallog_fp.txt'\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
    "        list(model.parameters()) + list(modelHeatmap.parameters()) +\\\n",
    "        list(modelLocmap.parameters()) ), lr=1.0e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    for idx, batch in enumerate(loader_train):\n",
    "        image = batch['image'].float()\n",
    "        pos2d_list = batch['pos_2d'] \n",
    "        pos3d_list = batch['pos_3d']\n",
    "        \n",
    "        model.train()\n",
    "        loss, loss_detailed = get_loss(model, modelHeatmap, modelLocmap,\n",
    "                        image, pos2d_list, pos3d_list, blw, params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        write_log(trainlog_fp, epoch, idx, loss, loss_detailed, 'train')\n",
    "        \n",
    "        if idx % print_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = []\n",
    "                val_loss_det = []\n",
    "                for vidx, vbatch in enumerate(loader_val):\n",
    "                    image = vbatch['image'].float()\n",
    "                    pos2d_list = vbatch['pos_2d'] \n",
    "                    pos3d_list = vbatch['pos_3d']\n",
    "                    \n",
    "                    vloss, vloss_detailed = get_loss(model, modelHeatmap, modelLocmap,\n",
    "                        image, pos2d_list, pos3d_list, blw, params)\n",
    "                    \n",
    "                    val_loss.append(vloss)\n",
    "                    val_loss_det.append(vloss_detailed)\n",
    "                    \n",
    "            val_loss = np.mean(val_loss)\n",
    "            val_loss_det = np.array(val_loss_det)\n",
    "            val_loss_det = np.mean(val_loss_det, axis=0)\n",
    "            write_log(vallog_fp, epoch, idx, val_loss, val_loss_det, 'val')\n",
    "                \n",
    "        if idx % save_every == 0:\n",
    "            save_model(epoch, idx, model, modelHeatmap, modelLocmap, optimizer)\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makePosList(h_pred, l_pred):\n",
    "    p2d_y, p2d_x = np.unravel_index(torch.argmax(h_pred.view(num_joints, -1), dim=1).data.numpy(), (image_size, image_size))\n",
    "    p2d = np.stack((p2d_x, p2d_y), axis=-1)\n",
    "\n",
    "    p3d_x = l_pred[0].data.numpy()\n",
    "    p3d_x = p3d_x[p2d_y, p2d_x]\n",
    "\n",
    "    p3d_y = l_pred[1].data.numpy()\n",
    "    p3d_y = p3d_y[p2d_y, p2d_x]\n",
    "\n",
    "    p3d_z = l_pred[2].data.numpy()\n",
    "    p3d_z = p3d_z[p2d_y, p2d_x]\n",
    "\n",
    "    p3d = np.stack((p3d_x, p3d_y, p3d_z), axis=-1)\n",
    "    \n",
    "    return p2d, p3d\n",
    "#b_idx = torch.from_numpy(np.repeat(np.arange(batch_size), num_joints)).long()\n",
    "mplb_idx = np.repeat(np.arange(batch_size), num_joints)\n",
    "def makePosListBatch(h_pred, l_pred):\n",
    "    idx_2d = torch.argmax(h_pred.view(batch_size, num_joints, -1), dim=2).data.numpy()\n",
    "    \n",
    "    p2d_y, p2d_x = np.unravel_index(idx_2d, (image_size, image_size))\n",
    "    p2d = np.stack((p2d_x, p2d_y), axis=-1)\n",
    "\n",
    "    l_pred = l_pred.view(batch_size, 3, -1)\n",
    "    \n",
    "    p3d_x = l_pred[:, 0].data.numpy()\n",
    "    p3d_x = p3d_x[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d_y = l_pred[:, 1].data.numpy()\n",
    "    p3d_y = p3d_y[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d_z = l_pred[:, 2].data.numpy()\n",
    "    p3d_z = p3d_z[mplb_idx, idx_2d.reshape(-1)]\n",
    "\n",
    "    p3d = np.stack((p3d_x.reshape(batch_size, num_joints), p3d_y.reshape(batch_size, num_joints), p3d_z.reshape(batch_size, num_joints)), axis=-1)\n",
    "    \n",
    "    return p2d, p3d\n",
    "\n",
    "print(\"Evaluation...\")\n",
    "\n",
    "### IMPORTANT ###\n",
    "# Switch all models to \"eval\" mode so BatchNorm stop computing new mean and variance, and dropout no longer dropout\n",
    "# Reference here: https://discuss.pytorch.org/t/what-does-model-eval-do-for-batchnorm-layer/7146\n",
    "# An here: https://pytorch.org/docs/0.3.1/nn.html?highlight=eval#torch.nn.Module.eval\n",
    "model.eval()\n",
    "modelHeatmap.eval()\n",
    "modelLocmap.eval()\n",
    "\n",
    "eval_loss = 0\n",
    "eval_iter = 0\n",
    "def eval_net():\n",
    "    for idx, batch in enumerate(loader_train):\n",
    "        eval_iter = idx\n",
    "        image = batch['image'].float()\n",
    "        pos2d_list = batch['pos_2d'] # size (N, 21, 2)\n",
    "        pos3d_list = batch['pos_3d'] # size (N, 21, 3)\n",
    "        loc_map, heatmap, one_hot = makeMaps(pos2d_list, pos3d_list)\n",
    "        y_pred = model(image)\n",
    "        h_pred = modelHeatmap(y_pred)\n",
    "        l_pred = modelLocmap(y_pred)\n",
    "        #print(\"output shape: {}\".format(y_pred.shape))\n",
    "        # use heatmap loss defined in VNect\n",
    "        #loss = computeLoss(heatmap, one_hot, loc_map, h_pred, l_pred)\n",
    "\n",
    "        #eval_loss = eval_loss + loss\n",
    "\n",
    "        if idx == 0:\n",
    "            # show some images\n",
    "            p2d, p3d = makePosList(h_pred[0], l_pred[0])\n",
    "            show_joints(image[0].data.numpy().transpose((1,2,0)), p2d, p3d)\n",
    "            show_joints(image[0].data.numpy().transpose((1,2,0)), batch['pos_2d'][0], batch['pos_3d'][0])\n",
    "            break\n",
    "#eval_net()\n",
    "#eval_iter = eval_iter + 1\n",
    "#print(\"Eval Loss: {}\".format(eval_loss / eval_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fully Connected Layer to Predict Gesture**\n",
    "\n",
    "Input: The outputs of our joint prediction model outputs:\n",
    "\n",
    "p2d: # size (N, 21, 2)\n",
    "p3d: # size (N, 21, 3)\n",
    "\n",
    "Output:\n",
    "\n",
    "y: # size (N, C=10), where C is the number of gesture classes\n",
    "\n",
    "**Note:** We will use both the 2D positions and the 3D positions of the joints to figure out what the gesture is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, let's get the gesture data\n",
    "g_dataset = GestureDataset('gesture_dataset.csv', transform=transform, train=True)\n",
    "\n",
    "N = 40 #len(g_dataset)\n",
    "\n",
    "loader_g_train = DataLoader(g_dataset, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8))))\n",
    "\n",
    "loader_g_val = DataLoader(g_dataset, batch_size=batch_size,\n",
    "            sampler=sampler.SubsetRandomSampler(range(int(N*0.8), int(N*0.9))))\n",
    "\n",
    "loader_g_test = DataLoader(g_dataset, batch_size=batch_size,\n",
    "                         sampler=sampler.SubsetRandomSampler(range(int(N*0.9),N)))\n",
    "save_model(epoch, idx, model, modelHeatmap, modelLocmap, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our fc should just output probabilities\n",
    "norm5d = nn.InstanceNorm1d(num_features=5)\n",
    "\n",
    "fc = nn.Sequential(\n",
    "    nn.Linear(in_features=105, out_features=50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=50, out_features=10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# predicted gesture probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the FC loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines constants\n",
    "batch_idx = torch.from_numpy(np.arange(batch_size)).long()\n",
    "epsilon = 1e-8\n",
    "def compute_g_loss(g_pred, g_GT):\n",
    "    # g_GT is of size (N, ) it just contains the labels for the batch\n",
    "    # print(\"Probability sum (should be 1.0): \", torch.sum(g_pred[0]) )\n",
    "    # print(\"Probability max: \", torch.max(g_pred[0]), \" ### Probability min: \", torch.min(g_pred[0]))\n",
    "    # print(\"Label: \", g_GT.long() )\n",
    "    g_loss = torch.sum(-1.0 * (g_pred[batch_idx, g_GT.long()] + epsilon).log()) / batch_size\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freeze Joint Prediction Network and Define Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# completely freeze out JP Net# comple \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in modelHeatmap.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in modelLocmap.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimize fully connected network only\n",
    "g_optimizer = torch.optim.Adam(fc.parameters(), lr=1.0e-3)\n",
    "g_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load previously trained parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_param_e1_i4200.pt', map_location={'cuda:0': 'cpu'}))\n",
    "modelHeatmap.load_state_dict(torch.load('modelHeatmap_param_e1_i4200.pt', map_location={'cuda:0': 'cpu'}))\n",
    "modelLocmap.load_state_dict(torch.load('modelLocmap_param_e1_i4200.pt', map_location={'cuda:0': 'cpu'}))\n",
    "\n",
    "#fc.load_state_dict(torch.load('fc_param.pt'))\n",
    "#g_optimizer.load_state_dict(torch.load('g_optimizer_param.pt'))\n",
    "#g_training_param = torch.load('g_training_param.pt')\n",
    "#g_losses = training_param['g_losses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    for idx, batch in enumerate(loader_g_train):\n",
    "        # make all the ground truth tensors needed for loss computation\n",
    "        image = batch['image'].float()\n",
    "        # y_pred is of size 64 x 224 x 224\n",
    "        y_pred = model(image)\n",
    "        \n",
    "        # h_pred is of size 21 x 224 x 224\n",
    "        h_pred = modelHeatmap(y_pred)\n",
    "        \n",
    "        # l_pred is of size 3 x 224 x 224, the 3 representing x, y, z location maps of all 21 joints\n",
    "        l_pred = modelLocmap(y_pred)\n",
    "        \n",
    "        p2d, p3d = makePosListBatch(h_pred, l_pred)\n",
    "        #print(p2d.shape, p3d.shape)\n",
    "        # print and store the loss curve\n",
    "        \n",
    "        ### Begin: Gesture recognition network\n",
    "        p2d = torch.from_numpy(p2d) # shape (N, 21, 2)\n",
    "        p3d = torch.from_numpy(p3d) # shape (N, 21, 3)\n",
    "        \n",
    "        p2d.transpose_(1, 2) # shape (N, 2, 21)\n",
    "        p3d.transpose_(1, 2) # shape (N, 3, 21)\n",
    "        \n",
    "        # put 2D and 3D joint positions together\n",
    "        fc_in = torch.cat((p2d.float(), p3d), dim=1) # shape (N, 5, 21)\n",
    "        \n",
    "        n_fc_in = norm5d(fc_in) # normalized each of 2dx, 2dy, 3dx, 3dy, 3dz, over 21 joints\n",
    "\n",
    "        n_fc_in = n_fc_in.view(batch_size, -1) # shape(N, 105)\n",
    "        \n",
    "        #fc_in = torch.randn_like(fc_in)\n",
    "        g_pred = fc(n_fc_in) # shape (N, 10)\n",
    "        ### End: Gesture recognition network\n",
    "        \n",
    "        g_loss = compute_g_loss(g_pred, batch['label'])\n",
    "        \n",
    "        print(\"G Loss: {}\".format(g_loss))\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        \n",
    "        g_optimizer.step()\n",
    "        # Clears the gradients of all optimized torch.Tensor s\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "    torch.save(fc.state_dict(), 'fc_param.pt')\n",
    "    torch.save(g_optimizer.state_dict(), 'g_optimizer_param.pt')\n",
    "    torch.save({'g_losses': g_losses, 'epoch': epoch + 1}, 'g_training_param.pt')\n",
    "        \n",
    "print(\"======Training Done======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(loader_g_train):\n",
    "    # make all the ground truth tensors needed for loss computation\n",
    "    image = batch['image'].float()\n",
    "    # y_pred is of size 64 x 224 x 224\n",
    "    y_pred = model(image)\n",
    "\n",
    "    # h_pred is of size 21 x 224 x 224\n",
    "    h_pred = modelHeatmap(y_pred)\n",
    "\n",
    "    # l_pred is of size 3 x 224 x 224, the 3 representing x, y, z location maps of all 21 joints\n",
    "    l_pred = modelLocmap(y_pred)\n",
    "\n",
    "    p2d, p3d = makePosList(h_pred[0], l_pred[0])\n",
    "    show_joints(image[0].data.numpy().transpose((1,2,0)), p2d, p3d)\n",
    "    \n",
    "    p2d, p3d = makePosList(h_pred[1], l_pred[1])\n",
    "    show_joints(image[1].data.numpy().transpose((1,2,0)), p2d, p3d)\n",
    "    \n",
    "    p2d, p3d = makePosList(h_pred[2], l_pred[2])\n",
    "    show_joints(image[2].data.numpy().transpose((1,2,0)), p2d, p3d)\n",
    "    \n",
    "    p2d, p3d = makePosList(h_pred[3], l_pred[3])\n",
    "    show_joints(image[3].data.numpy().transpose((1,2,0)), p2d, p3d)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert PyTorch Model to Keras (Experimental and DOESN't work)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://mil-tokyo.github.io/webdnn/docs/tutorial/pytorch.html\n",
    "from pytorch2keras.converter import pytorch_to_keras\n",
    "\n",
    "# model = models.alexnet(pretrained=True)\n",
    "# graph = PyTorchConverter().convert(model, dummy_input)\n",
    "# exec_info = generate_descriptor(\"webgpu\", graph)  # also \"webassembly\", \"webgl\", \"fallback\" are available.\n",
    "# exec_info.save(\"./output\")\n",
    "\n",
    "dummy_input = torch.autograd.Variable(torch.randn(1, 3, 224, 224))\n",
    "dummy_base_output = torch.autograd.Variable(torch.randn(1, 128, 224, 224))\n",
    "\n",
    "# important to switch all models to eval mode\n",
    "model.eval()\n",
    "modelHeatmap.eval()\n",
    "modelLocmap.eval()\n",
    " \n",
    "# we should specify shape of the input tensor\n",
    "# Export base model\n",
    "k_model = pytorch_to_keras(model, dummy_input, (3, 224, 224,), verbose=True)\n",
    "k_model.save('k_model.h5')\n",
    "\n",
    "# Export 2D heatmap branch\n",
    "k_modelHeatmap = pytorch_to_keras(modelHeatmap, dummy_base_output, (128, 224, 224,), verbose=True)\n",
    "k_modelHeatmap.save('k_modelHeatmap.h5')\n",
    "\n",
    "# Export 3D location branch\n",
    "k_modelLocmap = pytorch_to_keras(modelLocmap, dummy_base_output, (128, 224, 224,), verbose=True)\n",
    "k_modelLocmap.save('k_modelLocmap.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50 25557032\n",
      "model 9553920\n",
      "modelHeatmap 24213\n",
      "modelLocmap 97923\n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50()\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"resnet50\", count_parameters(resnet50))\n",
    "print(\"model\", count_parameters(model))\n",
    "print(\"modelHeatmap\", count_parameters(modelHeatmap))\n",
    "print(\"modelLocmap\", count_parameters(modelLocmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
